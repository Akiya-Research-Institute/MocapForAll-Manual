{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is MocapForAll? Motion capture software for everyone No special equipment required You can capture human motion if you have the followings: a middle range PC 2 or more webcams a room of about 2.5m x 2.5m You can use regular webcams like the ones used in video conferences. You can also use apps that turns your smartphones or tablets into webcams. Realtime with a middle range PC For examples, it runs at around 17 fps on Surface Pro 7 which does not have a dedicated GPU 30 to 60 fps on GTX 1080 Ti Cheap An option for projects with limited budgets, such as indie games, indie films, or education. What you can do with MocapForAll You can output captured motion to the network(*1) via VMT protocol(*2) and VMC protocol (*3) in real time. You can save the captured motion to files in BVH format and FBX format. You can output captured motion to the shared memory in real time. Note (*1) Both VMT protocol and VMC protocol use UDP/OpenSound Control. (*2) \"VMT protocol\" here refers to the message format used in the communication of Virtual Motion Tracker . The official HP of Virtual Motion Tracker does not use the word \"VMT protocol\", but MocapForAll uses the word \"VMT protocol\" for convenience. (*3) \"VMC protocol\" is a message format used for communication between applications such as VirtualMotionCapture . Note that VirtualMotionCapture itself is not required for other compatible apps to communicate with each other using VMC protocol. These allow you to do the followings: Use in SteamVR via Virtual Motion Tracker Through Virtual Motion Tracker , the capture motion can be used as virtual trackers in applications running on SteamVR. Use in Unreal Engine4, Unreal Engine5, Unity You can send the captured motion to Unreal Engine4, Unreal Engine5, or Unity for game development or video production. Plugins for linking data directly to UE4, UE5, or Unity are available here . As described in Use in other apps via VMC protocol , it is also possible to link by EVMC4U and VMC4UE using VMC Protocol. A Unity sample to read data from the shared memory written by MocapForAll is available here . Use in other apps via VMC protocol You can send the captured motion to various applications via the VMC protocol. The following are confirmed to work: Sending bones and facial expression morphs to Unity using EVMC4U Sending bones and facial expression morphs to Unreal Engine using VMC4UE Sending bones to Blender using VMC4B Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending tracker to VirtualMotionCapture Save animations to files You can save the captured motion as BVH files and FBX files. It can be used with Blender etc. Create programs to receive data As the output specifications are open to public (except for FBX), you can even create your own programs to receive data from MocapForAll.","title":"What is MocapForAll?"},{"location":"#what-is-mocapforall","text":"","title":"What is MocapForAll?"},{"location":"#motion-capture-software-for-everyone","text":"","title":"Motion capture software for everyone"},{"location":"#no-special-equipment-required","text":"You can capture human motion if you have the followings: a middle range PC 2 or more webcams a room of about 2.5m x 2.5m You can use regular webcams like the ones used in video conferences. You can also use apps that turns your smartphones or tablets into webcams.","title":"No special equipment required"},{"location":"#realtime-with-a-middle-range-pc","text":"For examples, it runs at around 17 fps on Surface Pro 7 which does not have a dedicated GPU 30 to 60 fps on GTX 1080 Ti","title":"Realtime with a middle range PC"},{"location":"#cheap","text":"An option for projects with limited budgets, such as indie games, indie films, or education.","title":"Cheap"},{"location":"#what-you-can-do-with-mocapforall","text":"You can output captured motion to the network(*1) via VMT protocol(*2) and VMC protocol (*3) in real time. You can save the captured motion to files in BVH format and FBX format. You can output captured motion to the shared memory in real time. Note (*1) Both VMT protocol and VMC protocol use UDP/OpenSound Control. (*2) \"VMT protocol\" here refers to the message format used in the communication of Virtual Motion Tracker . The official HP of Virtual Motion Tracker does not use the word \"VMT protocol\", but MocapForAll uses the word \"VMT protocol\" for convenience. (*3) \"VMC protocol\" is a message format used for communication between applications such as VirtualMotionCapture . Note that VirtualMotionCapture itself is not required for other compatible apps to communicate with each other using VMC protocol. These allow you to do the followings:","title":"What you can do with MocapForAll"},{"location":"#use-in-steamvr-via-virtual-motion-tracker","text":"Through Virtual Motion Tracker , the capture motion can be used as virtual trackers in applications running on SteamVR.","title":"Use in SteamVR via Virtual Motion Tracker"},{"location":"#use-in-unreal-engine4-unreal-engine5-unity","text":"You can send the captured motion to Unreal Engine4, Unreal Engine5, or Unity for game development or video production. Plugins for linking data directly to UE4, UE5, or Unity are available here . As described in Use in other apps via VMC protocol , it is also possible to link by EVMC4U and VMC4UE using VMC Protocol. A Unity sample to read data from the shared memory written by MocapForAll is available here .","title":"Use in Unreal Engine4, Unreal Engine5, Unity"},{"location":"#use-in-other-apps-via-vmc-protocol","text":"You can send the captured motion to various applications via the VMC protocol. The following are confirmed to work: Sending bones and facial expression morphs to Unity using EVMC4U Sending bones and facial expression morphs to Unreal Engine using VMC4UE Sending bones to Blender using VMC4B Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending tracker to VirtualMotionCapture","title":"Use in other apps via VMC protocol"},{"location":"#save-animations-to-files","text":"You can save the captured motion as BVH files and FBX files. It can be used with Blender etc.","title":"Save animations to files"},{"location":"#create-programs-to-receive-data","text":"As the output specifications are open to public (except for FBX), you can even create your own programs to receive data from MocapForAll.","title":"Create programs to receive data"},{"location":"acknowledgement/","text":"Acknowledgements We express our sincere thanks to Gonzales k\u00e9vin, \"Raybox41130\" for the French translation.","title":"Acknowledgements"},{"location":"acknowledgement/#acknowledgements","text":"We express our sincere thanks to Gonzales k\u00e9vin, \"Raybox41130\" for the French translation.","title":"Acknowledgements"},{"location":"changelog/","text":"Changelog v1.18.1 (7 Feb 2023) Added a funtion to send tracking positions to VRChat as OSC Tracker . v1.17.0 (10 May 2022) Fixed a bug that the last camera does not work properly in \"Precision mode\" when using an odd number of cameras. v1.16.0 (13 Mar. 2022) Added UE Mannequin characters. Added a function to export motion data as a FBX file. v1.15.0 (19 Dec. 2021) Updated the default map's background and lighting to make it easier to see. Added a function to stop rendering of the viewport. Added a function to preview the body tracking result in 2D. Added a function to stop the motion of lower body of the character. Fixed a bug that the app crashes when BVH export started if video is used and \"Capture face\" is on. v1.14.0 (29 Oct. 2021) Updated the AI for hand tracking. (MediaPipe Hand latest version) Added a function to preview the hand tracking result in 2D. Added a function to delay the input of camera in order to match the timing of the cameras. (You need to specify the delay in milliseconds by hand. So, it is not much practical yet.) Changed the camera preview UI to keep the aspect ratio of the image. Added a function to magnify the camera preview image. v1.13.3 (11 Oct. 2021) Fixed a bug that camera preview shows a white screen when DirectShow is used. Updated the French translation. v1.13.1 (6 Oct. 2021) Fixed a bug that shared memory settings were not saved correctly. Fixed a typo in the shared memory output specification. Removed unnecessary files. v1.13 (5 Oct. 2021) Added French translation. Added a function to export bone tranform and facial expression raw data to shared memory. Added a function to execute \"StartCapture\" and \"LoadAllCameras\" at startup when specified in command line arguments. (Example: MocapForAll.exe StartCapture LoadAllCameras) Fixed a bug that the VMT trackers for hands were not sent when \"As controllers\" is turned off. v1.12 (17 Sep. 2021) Changed the AI model for human detection at the start of tracking to the same model as v1.10 (MediaPipe Pose \u2192 YOLOv4). Added a function to capture motion from recorded videos. v1.11 (13 Sep. 2021) Changed the AI model for human detection at the start of tracking (YOLOv4 \u2192 MediaPipe Pose). The CPU/GPU usage is reduced. Added a function to send VMTs of hands as controllers. Added options to adjust offsets of virtual tracker positions for all body parts. Added buttons next to the scroll bars to adjust the values. Added a function to display the camera name on the viewport. Added a function to automatically set the language when the app started for the first time. (To fix the problem that numbers cannot be handled correctly in some languages.) Fixed a bug that \"Align coord to VR\" button always appears when the app launched. v1.10 (4 Sep. 2021) Added a function to automatically align the coordinates of SteamVR and MocapForAll . v1.9.1 (23 Aug. 2021) Added MetaHuman models of various body types. v1.9 (21 Aug. 2021) Added Japanese translation. Added help displayed by mouse over. Added a function to move the viewpoint with the mouse wheel and drag while holding down the middle mouse button. v1.8 (17 Aug. 2021) Fixed a bug that v1.6 updates were not included in v1.7. Fixed a bug that rotation of Root bone sent by VMC protocol was invalid. This fixes the issue that VMC4UE was unable to receive motion. Fixed a bug that the feet sticked to the ground when using a short character. (Changed to automatically adjust the ground contact strength of the foot according to the scale of the lower body.) Fixed a bug that hip bone name in BVH file is not based on the model. Added an option to export BVH file without root motion. Removed the option to export BVH file with root which only moves horizontally. v1.7 (7 Aug. 2021) Updated the AI used in \"Speed\" and \"Speed+\" modes. (From Movenet V1 to V4) Added a function to automatically adjust the position and the orientation of the ground. Added a function to automatically calibrate the camera extrinsic parameter by using human motion. v1.6 (31 Jul. 2021) Fixed a bug that the first letters of bone names in VMC protocol were not uppercase. (This change fixes the problem that some apps such as VSeeFace could not receive bone data from MocapForAll.) Changed the messages sent by VMC protocol. The following messages are sent additionally: /VMC/Ext/Root/Pos (The value is always 0 (origin)) /VMC/Ext/OK (The value is always 1) /VMC/Ext/T /VMC/Ext/Tra/Pos (This allows some apps such as VirtualMotionCapture to receive data from MocapForAll as trackers.) Devided the bundle (packet) of VMC protocol data so that it does not exceed 1500 bytes. Fixed a bug that settings about receiving VMC protocol are not saved. Fixed a bug that VMC protocol data could not be received after changing character. Added an option to add the root bone to the bone hierarchy in BVH file. v1.5 (22 Jul. 2021) Added a function to export motion data via VMC protocol as a \"Performer\". Added a function to receeve facial morph data from \"Assistant\" via VMC protocol, and send it as a \"Performer\". Added a function to automatically load the VRM file which was loaded by the user last time. Added a function to show warnings when calibration results are doubtful. v1.4.1 (18 Jul. 2021) Added a function to export motion data as a BVH file. Added a function to save and load whole camera setup. Improved animation retarget for VRM models. Fixed a bug that some cameras do not work in \"UE4 Media Player\" mode. Fixed a bug that Chroma-key map cannot be loaded if Apendix2 is not installed. v1.3 (10 Jul. 2021) Changed the default library for webcam operation. The new library uses UE4's Media Player, which improves performance at high resolutions, but (probably) makes virtual cameras unusable. If you want to use virtual cameras, select \"DirectShow\" from the pull-down menu of \"Add Camera\". Added a function to select the GPU to be used for calculation. Added a button to reset the detected marker position. Added a function to reset the view of the 3D viewport. Added a function to make the view of the 3D viewport follow the character. Fixed a bug that the offset value of the virtual tracker for the right foot was not loaded correctly at startup. Changed the display method of FPS. Changed the installatioin method. Precision mode, HDRI maps, MetaHuman Character, and TensorRT mode are now optional. Network Installer was added for easy installation. v1.2 (3 Jul. 2021) Added a function to display the position of the cameras in 3D viewport. Added a function to calibrate camera extrinsic parameters using multiple ArUco markers. Added a function to calibrate camera extrinsic parameters using multiple ChArUco diamond markers. Added a function to input the marker size, which is used to define the scale of tracking space. v1.1 (25 Jun. 2021) GPU_CUDA mode is removed. GPU_DirectML mode is added. DirectML provides GPU acceleration on DirectX 12 capable GPUs. Supported GPUs include: NVIDIA Kepler (GTX 600 series) and above AMD GCN 1st Gen (Radeon HD 7000 series) and above CPU version and GPU version are integrated. v1.0.1 (18 Jun. 2021) Added a function to flip the camera image. v1.0 (17 Jun. 2021) No change from v0.2 (To match the version number with the full version) v0.2 Camera Changed the library for webcam manipulation from OpenCV to DirectShow. Added a display for camera name. Added a function to change camera image size. Calibration Added a function to reset the calibration information when user clicks \"Start\" in calibration panel. Added a function to reset the calibration information when user changes camera. Data export Added a function to automatically enable VMT's option \"SetAutoPoseUpdate\" when user clicks \"As relative position to HMD\" Removed the text box to specify HMD serial number. (vmt_010 and before are not supported any longer.) Added options to adjust offsets for virtual trackers of pelvis and feet. UI Added a scroll bar in the Settings menu. Added a limitation that some options can be changed only when capture is stopped. Changed the max value from 2 to 5 of the slider in \"Settings > Coordinates > Scales\". Removed the function to draw tracking positions on camera preview. Installation Added a unzip command file for installation. Removed *.pdb which is not necessary to run the app.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v1181-7-feb-2023","text":"Added a funtion to send tracking positions to VRChat as OSC Tracker .","title":"v1.18.1 (7 Feb 2023)"},{"location":"changelog/#v1170-10-may-2022","text":"Fixed a bug that the last camera does not work properly in \"Precision mode\" when using an odd number of cameras.","title":"v1.17.0 (10 May 2022)"},{"location":"changelog/#v1160-13-mar-2022","text":"Added UE Mannequin characters. Added a function to export motion data as a FBX file.","title":"v1.16.0 (13 Mar. 2022)"},{"location":"changelog/#v1150-19-dec-2021","text":"Updated the default map's background and lighting to make it easier to see. Added a function to stop rendering of the viewport. Added a function to preview the body tracking result in 2D. Added a function to stop the motion of lower body of the character. Fixed a bug that the app crashes when BVH export started if video is used and \"Capture face\" is on.","title":"v1.15.0 (19 Dec. 2021)"},{"location":"changelog/#v1140-29-oct-2021","text":"Updated the AI for hand tracking. (MediaPipe Hand latest version) Added a function to preview the hand tracking result in 2D. Added a function to delay the input of camera in order to match the timing of the cameras. (You need to specify the delay in milliseconds by hand. So, it is not much practical yet.) Changed the camera preview UI to keep the aspect ratio of the image. Added a function to magnify the camera preview image.","title":"v1.14.0 (29 Oct. 2021)"},{"location":"changelog/#v1133-11-oct-2021","text":"Fixed a bug that camera preview shows a white screen when DirectShow is used. Updated the French translation.","title":"v1.13.3 (11 Oct. 2021)"},{"location":"changelog/#v1131-6-oct-2021","text":"Fixed a bug that shared memory settings were not saved correctly. Fixed a typo in the shared memory output specification. Removed unnecessary files.","title":"v1.13.1 (6 Oct. 2021)"},{"location":"changelog/#v113-5-oct-2021","text":"Added French translation. Added a function to export bone tranform and facial expression raw data to shared memory. Added a function to execute \"StartCapture\" and \"LoadAllCameras\" at startup when specified in command line arguments. (Example: MocapForAll.exe StartCapture LoadAllCameras) Fixed a bug that the VMT trackers for hands were not sent when \"As controllers\" is turned off.","title":"v1.13 (5 Oct. 2021)"},{"location":"changelog/#v112-17-sep-2021","text":"Changed the AI model for human detection at the start of tracking to the same model as v1.10 (MediaPipe Pose \u2192 YOLOv4). Added a function to capture motion from recorded videos.","title":"v1.12 (17 Sep. 2021)"},{"location":"changelog/#v111-13-sep-2021","text":"Changed the AI model for human detection at the start of tracking (YOLOv4 \u2192 MediaPipe Pose). The CPU/GPU usage is reduced. Added a function to send VMTs of hands as controllers. Added options to adjust offsets of virtual tracker positions for all body parts. Added buttons next to the scroll bars to adjust the values. Added a function to display the camera name on the viewport. Added a function to automatically set the language when the app started for the first time. (To fix the problem that numbers cannot be handled correctly in some languages.) Fixed a bug that \"Align coord to VR\" button always appears when the app launched.","title":"v1.11 (13 Sep. 2021)"},{"location":"changelog/#v110-4-sep-2021","text":"Added a function to automatically align the coordinates of SteamVR and MocapForAll .","title":"v1.10 (4 Sep. 2021)"},{"location":"changelog/#v191-23-aug-2021","text":"Added MetaHuman models of various body types.","title":"v1.9.1 (23 Aug. 2021)"},{"location":"changelog/#v19-21-aug-2021","text":"Added Japanese translation. Added help displayed by mouse over. Added a function to move the viewpoint with the mouse wheel and drag while holding down the middle mouse button.","title":"v1.9 (21 Aug. 2021)"},{"location":"changelog/#v18-17-aug-2021","text":"Fixed a bug that v1.6 updates were not included in v1.7. Fixed a bug that rotation of Root bone sent by VMC protocol was invalid. This fixes the issue that VMC4UE was unable to receive motion. Fixed a bug that the feet sticked to the ground when using a short character. (Changed to automatically adjust the ground contact strength of the foot according to the scale of the lower body.) Fixed a bug that hip bone name in BVH file is not based on the model. Added an option to export BVH file without root motion. Removed the option to export BVH file with root which only moves horizontally.","title":"v1.8 (17 Aug. 2021)"},{"location":"changelog/#v17-7-aug-2021","text":"Updated the AI used in \"Speed\" and \"Speed+\" modes. (From Movenet V1 to V4) Added a function to automatically adjust the position and the orientation of the ground. Added a function to automatically calibrate the camera extrinsic parameter by using human motion.","title":"v1.7 (7 Aug. 2021)"},{"location":"changelog/#v16-31-jul-2021","text":"Fixed a bug that the first letters of bone names in VMC protocol were not uppercase. (This change fixes the problem that some apps such as VSeeFace could not receive bone data from MocapForAll.) Changed the messages sent by VMC protocol. The following messages are sent additionally: /VMC/Ext/Root/Pos (The value is always 0 (origin)) /VMC/Ext/OK (The value is always 1) /VMC/Ext/T /VMC/Ext/Tra/Pos (This allows some apps such as VirtualMotionCapture to receive data from MocapForAll as trackers.) Devided the bundle (packet) of VMC protocol data so that it does not exceed 1500 bytes. Fixed a bug that settings about receiving VMC protocol are not saved. Fixed a bug that VMC protocol data could not be received after changing character. Added an option to add the root bone to the bone hierarchy in BVH file.","title":"v1.6 (31 Jul. 2021)"},{"location":"changelog/#v15-22-jul-2021","text":"Added a function to export motion data via VMC protocol as a \"Performer\". Added a function to receeve facial morph data from \"Assistant\" via VMC protocol, and send it as a \"Performer\". Added a function to automatically load the VRM file which was loaded by the user last time. Added a function to show warnings when calibration results are doubtful.","title":"v1.5 (22 Jul. 2021)"},{"location":"changelog/#v141-18-jul-2021","text":"Added a function to export motion data as a BVH file. Added a function to save and load whole camera setup. Improved animation retarget for VRM models. Fixed a bug that some cameras do not work in \"UE4 Media Player\" mode. Fixed a bug that Chroma-key map cannot be loaded if Apendix2 is not installed.","title":"v1.4.1 (18 Jul. 2021)"},{"location":"changelog/#v13-10-jul-2021","text":"Changed the default library for webcam operation. The new library uses UE4's Media Player, which improves performance at high resolutions, but (probably) makes virtual cameras unusable. If you want to use virtual cameras, select \"DirectShow\" from the pull-down menu of \"Add Camera\". Added a function to select the GPU to be used for calculation. Added a button to reset the detected marker position. Added a function to reset the view of the 3D viewport. Added a function to make the view of the 3D viewport follow the character. Fixed a bug that the offset value of the virtual tracker for the right foot was not loaded correctly at startup. Changed the display method of FPS. Changed the installatioin method. Precision mode, HDRI maps, MetaHuman Character, and TensorRT mode are now optional. Network Installer was added for easy installation.","title":"v1.3 (10 Jul. 2021)"},{"location":"changelog/#v12-3-jul-2021","text":"Added a function to display the position of the cameras in 3D viewport. Added a function to calibrate camera extrinsic parameters using multiple ArUco markers. Added a function to calibrate camera extrinsic parameters using multiple ChArUco diamond markers. Added a function to input the marker size, which is used to define the scale of tracking space.","title":"v1.2 (3 Jul. 2021)"},{"location":"changelog/#v11-25-jun-2021","text":"GPU_CUDA mode is removed. GPU_DirectML mode is added. DirectML provides GPU acceleration on DirectX 12 capable GPUs. Supported GPUs include: NVIDIA Kepler (GTX 600 series) and above AMD GCN 1st Gen (Radeon HD 7000 series) and above CPU version and GPU version are integrated.","title":"v1.1 (25 Jun. 2021)"},{"location":"changelog/#v101-18-jun-2021","text":"Added a function to flip the camera image.","title":"v1.0.1 (18 Jun. 2021)"},{"location":"changelog/#v10-17-jun-2021","text":"No change from v0.2 (To match the version number with the full version)","title":"v1.0 (17 Jun. 2021)"},{"location":"changelog/#v02","text":"Camera Changed the library for webcam manipulation from OpenCV to DirectShow. Added a display for camera name. Added a function to change camera image size. Calibration Added a function to reset the calibration information when user clicks \"Start\" in calibration panel. Added a function to reset the calibration information when user changes camera. Data export Added a function to automatically enable VMT's option \"SetAutoPoseUpdate\" when user clicks \"As relative position to HMD\" Removed the text box to specify HMD serial number. (vmt_010 and before are not supported any longer.) Added options to adjust offsets for virtual trackers of pelvis and feet. UI Added a scroll bar in the Settings menu. Added a limitation that some options can be changed only when capture is stopped. Changed the max value from 2 to 5 of the slider in \"Settings > Coordinates > Scales\". Removed the function to draw tracking positions on camera preview. Installation Added a unzip command file for installation. Removed *.pdb which is not necessary to run the app.","title":"v0.2"},{"location":"faq/","text":"FAQ The camera does not work Select UE4 media player in Select camera control framework Try selecting different formats in \"Image size\" Select Direct Show in Select camera control framework Select proper image size based on the camera specification If none of these work, unfortunately MocapForAll will not be able to use the camera. Multiple cameras of the same model are not recognized When connecting multiple cameras of the same model to a PC, there is a problem that some of them are not displayed (not detected) in the pull-down of camera selection in the app. In that case, try setting an arbitrary FriendlyName from the registry editor to the camera device that is not displayed. The method of setting / changing the FriendlyName differs depending on your environment, so please google it yourself. Since you are editing the registry, it is recommended to make a backup. Virtual camera is not recognized The default virtual camera in OBS does not work. Install OBS-VirtualCam plugin and select Direct Show in camera control framework . Can I use a black-white camera? No, you can't use a monochrome camera with this app. Please use an RGB camera. AR marker is not recognized Make sure the camera image is not a mirror image. Make sure the camera image is in focus on the AR marker. Make sure the AR marker is large enough in the camera image. Printing a larger size of the AR marker may help For camera calibration of extrinsic parameters, see If the marker cannot be read properly section. Camera calibration of extrinsic parameters does not work See If the marker cannot be read properly section. It does not capture even after camera calibration At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera. Make sure that two or more cameras that have been calibrated can see your whole body. It will not work if the final frame rate is below 4 fps. Check the frame rate . Does it support capturing multi-person? No, currently only one person can be captured. Therefore, only one person should be shown on the camera. VMT doesn't work Check errors in VMT Manager. Especially, check if Room Matrix is set. The motion of Virtual trackers are wrong Make sure that the alignment of coordinates between SteamVR and MocapForAll is correct. There is no serial number input field Please install the latest MocapForAll. From version 1.0 and above, you do not need to input the HMD serial number. Difference between MocapForAll and others OpenPose \uff1a The basic idea is similar, but the licenses are very different. Basically, you can use OpenPose only for \"ACADEMIC OR NON-PROFIT ORGANIZATION NONCOMMERCIAL RESEARCH USE ONLY\"","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#the-camera-does-not-work","text":"Select UE4 media player in Select camera control framework Try selecting different formats in \"Image size\" Select Direct Show in Select camera control framework Select proper image size based on the camera specification If none of these work, unfortunately MocapForAll will not be able to use the camera.","title":"The camera does not work"},{"location":"faq/#multiple-cameras-of-the-same-model-are-not-recognized","text":"When connecting multiple cameras of the same model to a PC, there is a problem that some of them are not displayed (not detected) in the pull-down of camera selection in the app. In that case, try setting an arbitrary FriendlyName from the registry editor to the camera device that is not displayed. The method of setting / changing the FriendlyName differs depending on your environment, so please google it yourself. Since you are editing the registry, it is recommended to make a backup.","title":"Multiple cameras of the same model are not recognized"},{"location":"faq/#virtual-camera-is-not-recognized","text":"The default virtual camera in OBS does not work. Install OBS-VirtualCam plugin and select Direct Show in camera control framework .","title":"Virtual camera is not recognized"},{"location":"faq/#can-i-use-a-black-white-camera","text":"No, you can't use a monochrome camera with this app. Please use an RGB camera.","title":"Can I use a black-white camera?"},{"location":"faq/#ar-marker-is-not-recognized","text":"Make sure the camera image is not a mirror image. Make sure the camera image is in focus on the AR marker. Make sure the AR marker is large enough in the camera image. Printing a larger size of the AR marker may help For camera calibration of extrinsic parameters, see If the marker cannot be read properly section.","title":"AR marker is not recognized"},{"location":"faq/#camera-calibration-of-extrinsic-parameters-does-not-work","text":"See If the marker cannot be read properly section.","title":"Camera calibration of extrinsic parameters does not work"},{"location":"faq/#it-does-not-capture-even-after-camera-calibration","text":"At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera. Make sure that two or more cameras that have been calibrated can see your whole body. It will not work if the final frame rate is below 4 fps. Check the frame rate .","title":"It does not capture even after camera calibration"},{"location":"faq/#does-it-support-capturing-multi-person","text":"No, currently only one person can be captured. Therefore, only one person should be shown on the camera.","title":"Does it support capturing multi-person?"},{"location":"faq/#vmt-doesnt-work","text":"Check errors in VMT Manager. Especially, check if Room Matrix is set.","title":"VMT doesn't work"},{"location":"faq/#the-motion-of-virtual-trackers-are-wrong","text":"Make sure that the alignment of coordinates between SteamVR and MocapForAll is correct.","title":"The motion of Virtual trackers are wrong"},{"location":"faq/#there-is-no-serial-number-input-field","text":"Please install the latest MocapForAll. From version 1.0 and above, you do not need to input the HMD serial number.","title":"There is no serial number input field"},{"location":"faq/#difference-between-mocapforall-and-others","text":"OpenPose \uff1a The basic idea is similar, but the licenses are very different. Basically, you can use OpenPose only for \"ACADEMIC OR NON-PROFIT ORGANIZATION NONCOMMERCIAL RESEARCH USE ONLY\"","title":"Difference between MocapForAll and others"},{"location":"how-to-capture/get-cameras/","text":"Get cameras You need webcams that can be connected to the PC in which MocapForAll installed. To use smartphones or tablets, use apps that turns your smartphones or tablets into webcams, such as DroidCam or Iriun . What kind of camera should I use? Webcams or smartphones There is no quantitative data on which is more accurate when using a webcam or smartphone. It is best to actually plug and test your device. When using battery-powered equipment such as smartphones, be careful not to run out of battery. For your information, all official videos of MocapForAll are taken using wirelessly connected smartphone and tablet, except those that state wired cameras are used. Wired or wireless The delay of motion capture by MocapForAll mainly consists of the time it takes for the camera image to be transferred to the PC. Wired cameras often have less delay than wireless cameras. Also, keep in mind that delays tend to be large when using apps that turn smartphones into webcams or virtual cameras. To use with VR devices, it is recommended to make the delay as small as possible. FOV, frame rate, image size The wider the FOV , the wider the space for motion capture. However, MocapForAll calculates the 3D position with a simple pinhole camera model, so if the image is distorted like a fisheye lens, the movement cannot be captured correctly. A camera with low distortion and a wide FOV is most suitable. Some people use 120 degree FOV cameras, and it seems they work very well. If the frame rate is low, that value can be a bottleneck in the frame rate of the capture. Considering the performance of your PC, get cameras with a higher frame rate than that of your desired capture result. The performance of MocapForAll itself is, using GTX1080Ti for example, about 60fps when MocapForAll is operated alone, and about 20fps when used with VR at the same time. Image size does not contribute much to the precision of the capture of body movement. About 640x480 pixels is enough. This is due to the pipeline of the image processing: firstly the image is cropped around the person and then reduced to, for example, 256x256 pixel to input to the AI. However, when capturing hands and faces, the same processing is performed for small areas of hands and face, so the larger the image size (and resolution), the better the accuracy. We usually use cameras with image sizes around HD. Where should I put cameras? Put cameras where they can see your whole body as well as the following conditions are satisfied: Vertical position: chest to eye level It is recommended to place the camera at the height between your chest and your eyes. If the camera is looking up or down too much, the accuracy tends to be poor. Horizontal position: 45\u00b0 left and right from front It is recommended to place the camera at 45 degree left and right from the front of the captured person. If the capture target and the two cameras are positioned in a straight line, the accuracy tend to be poor because the depth information is insufficient. This is an example of our camera placement: Number of the cameras By increasing the number of cameras, the occlusion is reduced, so it is expected to improve the accuracy. However, there is no quantitative data on this. Since the CPU / GPU usage increases almost in proportion to the number of cameras, increasing the number of cameras will reduce the frame rate of the capture depending on the performance of your PC. We recommend that you start with two cameras and consider adding more if you find that occlusion cause problems. \uff08Examples: Comparison of 4 cameras vs 2 cameras\uff09 Demonstration using 4 cameras. This app has no limit on the number of cameras. Keep in mind that CPU or GPU usage increases in proportion to the number of cameras. \u2193 2 cameras \u2193 https://t.co/x3ZmseEMpG pic.twitter.com/kbUyE2ZHNm \u2014 \u7a7a\u304d\u5bb6\u7dcf\u7814VR\u30e9\u30dc -Akiya Research Institute,VRlab- (@Akiya_Souken_VR) May 25, 2021","title":"Get cameras"},{"location":"how-to-capture/get-cameras/#get-cameras","text":"You need webcams that can be connected to the PC in which MocapForAll installed. To use smartphones or tablets, use apps that turns your smartphones or tablets into webcams, such as DroidCam or Iriun .","title":"Get cameras"},{"location":"how-to-capture/get-cameras/#what-kind-of-camera-should-i-use","text":"","title":"What kind of camera should I use?"},{"location":"how-to-capture/get-cameras/#webcams-or-smartphones","text":"There is no quantitative data on which is more accurate when using a webcam or smartphone. It is best to actually plug and test your device. When using battery-powered equipment such as smartphones, be careful not to run out of battery. For your information, all official videos of MocapForAll are taken using wirelessly connected smartphone and tablet, except those that state wired cameras are used.","title":"Webcams or smartphones"},{"location":"how-to-capture/get-cameras/#wired-or-wireless","text":"The delay of motion capture by MocapForAll mainly consists of the time it takes for the camera image to be transferred to the PC. Wired cameras often have less delay than wireless cameras. Also, keep in mind that delays tend to be large when using apps that turn smartphones into webcams or virtual cameras. To use with VR devices, it is recommended to make the delay as small as possible.","title":"Wired or wireless"},{"location":"how-to-capture/get-cameras/#fov-frame-rate-image-size","text":"The wider the FOV , the wider the space for motion capture. However, MocapForAll calculates the 3D position with a simple pinhole camera model, so if the image is distorted like a fisheye lens, the movement cannot be captured correctly. A camera with low distortion and a wide FOV is most suitable. Some people use 120 degree FOV cameras, and it seems they work very well. If the frame rate is low, that value can be a bottleneck in the frame rate of the capture. Considering the performance of your PC, get cameras with a higher frame rate than that of your desired capture result. The performance of MocapForAll itself is, using GTX1080Ti for example, about 60fps when MocapForAll is operated alone, and about 20fps when used with VR at the same time. Image size does not contribute much to the precision of the capture of body movement. About 640x480 pixels is enough. This is due to the pipeline of the image processing: firstly the image is cropped around the person and then reduced to, for example, 256x256 pixel to input to the AI. However, when capturing hands and faces, the same processing is performed for small areas of hands and face, so the larger the image size (and resolution), the better the accuracy. We usually use cameras with image sizes around HD.","title":"FOV, frame rate, image size"},{"location":"how-to-capture/get-cameras/#where-should-i-put-cameras","text":"Put cameras where they can see your whole body as well as the following conditions are satisfied:","title":"Where should I put cameras?"},{"location":"how-to-capture/get-cameras/#vertical-position-chest-to-eye-level","text":"It is recommended to place the camera at the height between your chest and your eyes. If the camera is looking up or down too much, the accuracy tends to be poor.","title":"Vertical position: chest to eye level"},{"location":"how-to-capture/get-cameras/#horizontal-position-45-left-and-right-from-front","text":"It is recommended to place the camera at 45 degree left and right from the front of the captured person. If the capture target and the two cameras are positioned in a straight line, the accuracy tend to be poor because the depth information is insufficient. This is an example of our camera placement:","title":"Horizontal position: 45\u00b0 left and right from front"},{"location":"how-to-capture/get-cameras/#number-of-the-cameras","text":"By increasing the number of cameras, the occlusion is reduced, so it is expected to improve the accuracy. However, there is no quantitative data on this. Since the CPU / GPU usage increases almost in proportion to the number of cameras, increasing the number of cameras will reduce the frame rate of the capture depending on the performance of your PC. We recommend that you start with two cameras and consider adding more if you find that occlusion cause problems. \uff08Examples: Comparison of 4 cameras vs 2 cameras\uff09 Demonstration using 4 cameras. This app has no limit on the number of cameras. Keep in mind that CPU or GPU usage increases in proportion to the number of cameras. \u2193 2 cameras \u2193 https://t.co/x3ZmseEMpG pic.twitter.com/kbUyE2ZHNm \u2014 \u7a7a\u304d\u5bb6\u7dcf\u7814VR\u30e9\u30dc -Akiya Research Institute,VRlab- (@Akiya_Souken_VR) May 25, 2021","title":"Number of the cameras"},{"location":"how-to-capture/adjust-settings/adjust-animation/","text":"Adjust animation The captured motion will be displayed on the screen after the following post processes. Whether to force feet to be grounded There is a function to control the positions of the feet so that they do not float in the air or sink into the floor. If the contact between feet and ground is important to you, turn on Settings > Animation Post Process > Force feet grouded . If the natural movement of the whole leg is important to you, turn off Settings > Animation Post Process > Force feet grouded . VR users can skip this Data before applying the above adjustment is exported when Settings > Data export > VMT protocol > Send tracking points is turned on. In other words, if you use with VMT and SteamVR, you do not need to care about this setting. If the feet stick to the ground The threshold of the distance between the foot and the ground to determine whether to apply this adjustment is automatically calculated using the value Settings > Coordinates > Scale > Lower body . If Prepare marker was not done properly and the scale of Lower body was too big, the character's feet may not be able to get off the ground at all. In that case, redo Prepare marker and Execute calibration , or turn off Force feet grounded . Adjust the smoothing There is a function to smooth the captured motion. After applying this smoothing, data is exported externally. Turn on/off the smoothing If the application that receive the captured motion performs its own smoothing, you may want to turn off the smoothing on the MocapForAll. You can do it by turning off Settings > Animation Post Process > Smoothing on body , Smoothing on finger and Smoothing on facial expression . Adjust the smoothing intensity You can change the intensity of the smoothing. Start the capture and observe the noise when standing upright without moving. If you want less noise, decrease the value of fc0 . Next, move your body at a speed which is appropriate for your purpose. If the captured motion cannot keep up with your body, increase the value of Beta . How it works One euro filter is used for the smoothing. One euro filter is simply \"low-pass filter whose cutoff frequency increases in proportion to speed\". Normal low-pass filter cannot keep up with quick movements when trying to suppress jitter. On the other hand, one euro filter can loosen noise suppression only for quick movements to ensure tracking for them, as well as suppressing noise at low speeds in the same way as normal low-pass filter. You can adjust the following 3 parameters: fc0 : Cutoff frequency at speed is zero. The smaller this value, the less noise at low speeds. Beta : How much the cutoff frequency increases in proportion to the speed. The higher this value, the quicker movement can be tracked. fcv : Cutoff frequency for speed. For speed, a normal lowpass filter with the cutoff frequency specified by this value is applied.","title":"Adjust animation"},{"location":"how-to-capture/adjust-settings/adjust-animation/#adjust-animation","text":"The captured motion will be displayed on the screen after the following post processes.","title":"Adjust animation"},{"location":"how-to-capture/adjust-settings/adjust-animation/#whether-to-force-feet-to-be-grounded","text":"There is a function to control the positions of the feet so that they do not float in the air or sink into the floor. If the contact between feet and ground is important to you, turn on Settings > Animation Post Process > Force feet grouded . If the natural movement of the whole leg is important to you, turn off Settings > Animation Post Process > Force feet grouded . VR users can skip this Data before applying the above adjustment is exported when Settings > Data export > VMT protocol > Send tracking points is turned on. In other words, if you use with VMT and SteamVR, you do not need to care about this setting. If the feet stick to the ground The threshold of the distance between the foot and the ground to determine whether to apply this adjustment is automatically calculated using the value Settings > Coordinates > Scale > Lower body . If Prepare marker was not done properly and the scale of Lower body was too big, the character's feet may not be able to get off the ground at all. In that case, redo Prepare marker and Execute calibration , or turn off Force feet grounded .","title":"Whether to force feet to be grounded"},{"location":"how-to-capture/adjust-settings/adjust-animation/#adjust-the-smoothing","text":"There is a function to smooth the captured motion. After applying this smoothing, data is exported externally.","title":"Adjust the smoothing"},{"location":"how-to-capture/adjust-settings/adjust-animation/#turn-onoff-the-smoothing","text":"If the application that receive the captured motion performs its own smoothing, you may want to turn off the smoothing on the MocapForAll. You can do it by turning off Settings > Animation Post Process > Smoothing on body , Smoothing on finger and Smoothing on facial expression .","title":"Turn on/off the smoothing"},{"location":"how-to-capture/adjust-settings/adjust-animation/#adjust-the-smoothing-intensity","text":"You can change the intensity of the smoothing. Start the capture and observe the noise when standing upright without moving. If you want less noise, decrease the value of fc0 . Next, move your body at a speed which is appropriate for your purpose. If the captured motion cannot keep up with your body, increase the value of Beta . How it works One euro filter is used for the smoothing. One euro filter is simply \"low-pass filter whose cutoff frequency increases in proportion to speed\". Normal low-pass filter cannot keep up with quick movements when trying to suppress jitter. On the other hand, one euro filter can loosen noise suppression only for quick movements to ensure tracking for them, as well as suppressing noise at low speeds in the same way as normal low-pass filter. You can adjust the following 3 parameters: fc0 : Cutoff frequency at speed is zero. The smaller this value, the less noise at low speeds. Beta : How much the cutoff frequency increases in proportion to the speed. The higher this value, the quicker movement can be tracked. fcv : Cutoff frequency for speed. For speed, a normal lowpass filter with the cutoff frequency specified by this value is applied.","title":"Adjust the smoothing intensity"},{"location":"how-to-capture/adjust-settings/adjust-scales/","text":"Adjust scales Click Start capture at the top of the window and let the cameras to see your whole body to capture your motion. By observing the captured result, adjust the scales as shown below to animate the character properly. VR users can skip this If you are using Virtual Motion Tracker and SteamVR, the scales will be adjusted in Align coordinates of SteamVR and MocapForAll section described later, so you can skip this section. Why there are 2 scales Game characters generally have longer legs than real humans, so the scale is divided into ones for upper and lower body. (Therefore, the ratio of the upper body scale to the lower body scale depends on the character you want to apply.) Lower body scale Affects to pelvis, hips, knees, feet, as well as the root position of the whole movement. If the character is always crunching, increase the value at Settings > Coordinates > Scales > Lower body . If the character is floating in the air, decrease the value at Settings > Coordinates > Scales > Lower body . Upper body scale Affects to chest, neck, head, shoulders, elbows, hands, fingers. If the character's shoulder is always facing down, increase the value at Settings > Coordinates > Scales > Upper body . If the character's shoulder is always facing up, decrease the value at Settings > Coordinates > Scales > Upper body .","title":"Adjust scales"},{"location":"how-to-capture/adjust-settings/adjust-scales/#adjust-scales","text":"Click Start capture at the top of the window and let the cameras to see your whole body to capture your motion. By observing the captured result, adjust the scales as shown below to animate the character properly. VR users can skip this If you are using Virtual Motion Tracker and SteamVR, the scales will be adjusted in Align coordinates of SteamVR and MocapForAll section described later, so you can skip this section. Why there are 2 scales Game characters generally have longer legs than real humans, so the scale is divided into ones for upper and lower body. (Therefore, the ratio of the upper body scale to the lower body scale depends on the character you want to apply.)","title":"Adjust scales"},{"location":"how-to-capture/adjust-settings/adjust-scales/#lower-body-scale","text":"Affects to pelvis, hips, knees, feet, as well as the root position of the whole movement. If the character is always crunching, increase the value at Settings > Coordinates > Scales > Lower body . If the character is floating in the air, decrease the value at Settings > Coordinates > Scales > Lower body .","title":"Lower body scale"},{"location":"how-to-capture/adjust-settings/adjust-scales/#upper-body-scale","text":"Affects to chest, neck, head, shoulders, elbows, hands, fingers. If the character's shoulder is always facing down, increase the value at Settings > Coordinates > Scales > Upper body . If the character's shoulder is always facing up, decrease the value at Settings > Coordinates > Scales > Upper body .","title":"Upper body scale"},{"location":"how-to-capture/adjust-settings/optimize-performance/","text":"Optimize performance Setup to reduce the CPU / GPU usage according to your environment and purpose. Check the frame rate Turn on Settings > Performance > Show frame rate to display current frame rate at the bottom of the window. Use GPU MocapForAll uses AI to estimate a person's pose. AI calculations are often faster on the GPU than on the CPU. If you have a GPU, it is highly recommended to select GPU_DriectML from Settings > General > Run DNN on . Select from multiple GPUs If you have multiple GPUs, you can also choose which GPU to use by Settings > General > Run DNN on > GPU . Should I use TensorRT? When using a GPU, you have two options, GPU_DriectML or GPU_TensorRT . TensorRT is difficult to install, takes a long time to load, and has little reward. So, it is recommended to use GPU_DriectML in many cases. GPU_DriectML: DirectML provides GPU acceleration on DirectX 12 capable GPUs. Examples of compatible hardware include: NVIDIA Kepler (GTX 600 series) and above AMD GCN 1st Gen (Radeon HD 7000 series) and above GPU_TensorRT: TensorRT provides GPU acceleration on supported NVIDIA GPUs. To use this, you need to install CUDA, cuDNN, TensorRT , and Appendix4_TensorRT_mode . Select capturing mode From Settings > General > Capture body , you can choose what to prioritize. In many cases, Speed is recommended. Speed+ is recommended when using a PC with low performance such as a laptop PC or when using together with a very heavy VR application. Use Precision mode when you want to capture motion precisely such as for movie production. You need to install Appendix1_Precision_mode . Reduce drawing With the following settings, you can reduce the drawing on MocapForAll and reduce the CPU / GPU usage. Select Empty character in Settings > General > Character . Select Minimum map in Settings > General > Map . Set small value (30% for example) and turn on Settings > Performance > Set screen percentage to , or turn off Settings > Performance > Render the viewport Limit the frame rate Set the target frame rate (30FPS for example) and turn on Settings > Performance > Limit framerate to: . MocapForAll runs up to this frame rate.","title":"Optimize performance"},{"location":"how-to-capture/adjust-settings/optimize-performance/#optimize-performance","text":"Setup to reduce the CPU / GPU usage according to your environment and purpose.","title":"Optimize performance"},{"location":"how-to-capture/adjust-settings/optimize-performance/#check-the-frame-rate","text":"Turn on Settings > Performance > Show frame rate to display current frame rate at the bottom of the window.","title":"Check the frame rate"},{"location":"how-to-capture/adjust-settings/optimize-performance/#use-gpu","text":"MocapForAll uses AI to estimate a person's pose. AI calculations are often faster on the GPU than on the CPU. If you have a GPU, it is highly recommended to select GPU_DriectML from Settings > General > Run DNN on . Select from multiple GPUs If you have multiple GPUs, you can also choose which GPU to use by Settings > General > Run DNN on > GPU . Should I use TensorRT? When using a GPU, you have two options, GPU_DriectML or GPU_TensorRT . TensorRT is difficult to install, takes a long time to load, and has little reward. So, it is recommended to use GPU_DriectML in many cases. GPU_DriectML: DirectML provides GPU acceleration on DirectX 12 capable GPUs. Examples of compatible hardware include: NVIDIA Kepler (GTX 600 series) and above AMD GCN 1st Gen (Radeon HD 7000 series) and above GPU_TensorRT: TensorRT provides GPU acceleration on supported NVIDIA GPUs. To use this, you need to install CUDA, cuDNN, TensorRT , and Appendix4_TensorRT_mode .","title":"Use GPU"},{"location":"how-to-capture/adjust-settings/optimize-performance/#select-capturing-mode","text":"From Settings > General > Capture body , you can choose what to prioritize. In many cases, Speed is recommended. Speed+ is recommended when using a PC with low performance such as a laptop PC or when using together with a very heavy VR application. Use Precision mode when you want to capture motion precisely such as for movie production. You need to install Appendix1_Precision_mode .","title":"Select capturing mode"},{"location":"how-to-capture/adjust-settings/optimize-performance/#reduce-drawing","text":"With the following settings, you can reduce the drawing on MocapForAll and reduce the CPU / GPU usage. Select Empty character in Settings > General > Character . Select Minimum map in Settings > General > Map . Set small value (30% for example) and turn on Settings > Performance > Set screen percentage to , or turn off Settings > Performance > Render the viewport","title":"Reduce drawing"},{"location":"how-to-capture/adjust-settings/optimize-performance/#limit-the-frame-rate","text":"Set the target frame rate (30FPS for example) and turn on Settings > Performance > Limit framerate to: . MocapForAll runs up to this frame rate.","title":"Limit the frame rate"},{"location":"how-to-capture/calibrate-cameras/connect-cameras/","text":"Connect cameras Add cameras Connect at least 2 cameras to your PC. Click Add camera button at the top of the MocapForAll window. Select the combo box next to Camera: to find the connected camera. You can change the image size of the camera by entering the dimensions and clicking Apply if camera supports the specified image size. If it fails to change image size Somtimes it fails to change the image size. In that case, please close camera, wait for a moment, and try again. You can flip the image horizontally. Flip image if necessary Some cameras have a mirror image by default. If the image is a mirror image, the AR marker cannot be read. You can rotate the image. Rotate camera properly At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera. Select camera control framework You can select the framework to control the camera by pressing the \u25bc next to Add camera at the top of the MocapForAll window. Direct show: Microsoft's media framework. You can use the OBS-VirtualCam plugin with this. UE4 media player: UE4's media framework. Better performance at high resolution. Some cameras don't work with this. Recorded video : Use recorded video instead of camera. See Motion capture from recorded videos . If the camera works with UE4 media player, it is recommended to use it. If it doesn't work, use Direct Show.","title":"Connect cameras"},{"location":"how-to-capture/calibrate-cameras/connect-cameras/#connect-cameras","text":"","title":"Connect cameras"},{"location":"how-to-capture/calibrate-cameras/connect-cameras/#add-cameras","text":"Connect at least 2 cameras to your PC. Click Add camera button at the top of the MocapForAll window. Select the combo box next to Camera: to find the connected camera. You can change the image size of the camera by entering the dimensions and clicking Apply if camera supports the specified image size. If it fails to change image size Somtimes it fails to change the image size. In that case, please close camera, wait for a moment, and try again. You can flip the image horizontally. Flip image if necessary Some cameras have a mirror image by default. If the image is a mirror image, the AR marker cannot be read. You can rotate the image. Rotate camera properly At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera.","title":"Add cameras"},{"location":"how-to-capture/calibrate-cameras/connect-cameras/#select-camera-control-framework","text":"You can select the framework to control the camera by pressing the \u25bc next to Add camera at the top of the MocapForAll window. Direct show: Microsoft's media framework. You can use the OBS-VirtualCam plugin with this. UE4 media player: UE4's media framework. Better performance at high resolution. Some cameras don't work with this. Recorded video : Use recorded video instead of camera. See Motion capture from recorded videos . If the camera works with UE4 media player, it is recommended to use it. If it doesn't work, use Direct Show.","title":"Select camera control framework"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/","text":"Execute calibration Get intrinsic parameters Display this image on your PC's screen as large as possible. In MocapForAll, click Start button under Camera > Calibration > Intrinsic . Take images with the camera from various angles for about 10 seconds. If the camera is already fixed, move the image instead of the camera. When calibration is completed, intrinsic parameters will be displayed on the app's screen and Intrinsic \u2611Calibrated will be shown. Save the camera calibration result from Save . Get extrinsic parameters The procedure differs depending on the 4 methods to get extrinsic parameters explained before. ChArUco board method ArUco cluster method Diamond cluster method Human motion method Place the image printed in Prepare markers section on the floor. This will be the origin of the captured motion. Place the cameras where they can see the placed image. In MocapForAll, select ChArUco board (default) in Settings > Calibration > Extrinsic calibration method . Click Start button under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the images printed in Prepare markers section on the floor. arucoMarker0.png will be the origin of the captured motion. Place the cameras where they can see the placed images. In MocapForAll, select ArUco cluster in Settings > Calibration > Extrinsic calibration method . Click Scan markers button under Camera > Calibration > Extrinsic . This will scan the relative positions of the markers. After a while, the markers will appear in the 3D viewport. After all markers appeared, click Stop scanning and then Start under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the images printed in Prepare markers section at a distance that fits in a frame of the camera. It does not have to be in the same plane. diamondMarker0.png will be the origin of the captured motion, so place this on the floor. In MocapForAll, select Diamond cluster in Settings > Calibration > Extrinsic calibration method . Click Scan markers button under Camera > Calibration > Extrinsic in one of the cameras. Take pictures of diamondMarker0.png and one of the markers at the same time. After a while, the position of another marker will be fixed with diamondMarker0.png as the origin, and the marker will be displayed on the 3D viewport. Repeat the above step for markers whose positions have been fixed and markers whose positions have not been fixed, and click Stop scanning when the positions of all markers have been fixed. Place the cameras where they can see at least one of the markers whose position is fixed. Click Start under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the cameras so that at least two of them can see your whole body at the same time. In MocapForAll, select Human motion in Settings > Calibration > Extrinsic calibration method . Select GPU_DirectML in Settings > General > Run DNN on if your hardware support it. (If you have already installed the GPU_TensorRT mode in the Appendix, you can also use it.\uff09 Select Speed in Settings > General > Capture body if your PC's performance is enough for it. (If you have already installed the Precision mode in the Appendix, you can also use it.\uff09 Click Start calibration button at the top of the window , and walk around on the floor where the cameras can see your whole body. Your motion will be captured and the positions of your joints will be used to find the cameras relative positions. After all cameras' extrinsic parameters calibrated, click the Find Ground button at the top of the window and walk around in the same way as If the marker cannot be read properly described below. This will determine the absolute position of the cameras. If the marker cannot be read properly Method to place marker on the wall Since the position of the marker will be the plane with zero height of the captured motion, it is basically recommended to place the marker on the floor, but it may be difficult to read the marker on the floor depending on the placement and specification of the camera. In that case, you can hang the marker on the wall to calibrate the camera and adjust the floor level later as follows: Replace the word \"on the floor\" with \"on the wall\" and execute the calibration procedure. Select GPU_DirectML in Settings > General > Run DNN on if your hardware support it. (If you have already installed the GPU_TensorRT mode in the Appendix, you can also use it.\uff09 Select Speed in Settings > General > Capture body if your PC's performance is enough for it. (If you have already installed the Precision mode in the Appendix, you can also use it.\uff09 Click the Find Ground button at the top of the window and walk around where at least 2 cameras can see your whole body. After a while, the position of the floor level will be adjusted automatically. Confirm the results If you have input the correct marker size in Prepare markers section, you can see the position of the camera in the 3D viewport. Use the WASD key and mouse drag to move the viewport on MocapForAll. Cameras may appear below the floor In this case, the calibration has failed. Save and load the results For each camera After calibration is completed, save the calibration result of each camera by clicking Save under Camera > Calibration > Intrinsic and Extrinsic . The saved result can be loaded by clicking Load button. For all cameras You can save and load the whole camera setup by clicking Save All Cameras and Load All Cameras buttons. Caution when saving all camera settings The camera selection is saved as the index inside the combo box. If you remove cameras from your PC, the order of the cameras in the combo box will change and you will not be able to load the cameras properly.","title":"Execute calibration"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#execute-calibration","text":"","title":"Execute calibration"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#get-intrinsic-parameters","text":"Display this image on your PC's screen as large as possible. In MocapForAll, click Start button under Camera > Calibration > Intrinsic . Take images with the camera from various angles for about 10 seconds. If the camera is already fixed, move the image instead of the camera. When calibration is completed, intrinsic parameters will be displayed on the app's screen and Intrinsic \u2611Calibrated will be shown. Save the camera calibration result from Save .","title":"Get intrinsic parameters"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#get-extrinsic-parameters","text":"The procedure differs depending on the 4 methods to get extrinsic parameters explained before. ChArUco board method ArUco cluster method Diamond cluster method Human motion method Place the image printed in Prepare markers section on the floor. This will be the origin of the captured motion. Place the cameras where they can see the placed image. In MocapForAll, select ChArUco board (default) in Settings > Calibration > Extrinsic calibration method . Click Start button under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the images printed in Prepare markers section on the floor. arucoMarker0.png will be the origin of the captured motion. Place the cameras where they can see the placed images. In MocapForAll, select ArUco cluster in Settings > Calibration > Extrinsic calibration method . Click Scan markers button under Camera > Calibration > Extrinsic . This will scan the relative positions of the markers. After a while, the markers will appear in the 3D viewport. After all markers appeared, click Stop scanning and then Start under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the images printed in Prepare markers section at a distance that fits in a frame of the camera. It does not have to be in the same plane. diamondMarker0.png will be the origin of the captured motion, so place this on the floor. In MocapForAll, select Diamond cluster in Settings > Calibration > Extrinsic calibration method . Click Scan markers button under Camera > Calibration > Extrinsic in one of the cameras. Take pictures of diamondMarker0.png and one of the markers at the same time. After a while, the position of another marker will be fixed with diamondMarker0.png as the origin, and the marker will be displayed on the 3D viewport. Repeat the above step for markers whose positions have been fixed and markers whose positions have not been fixed, and click Stop scanning when the positions of all markers have been fixed. Place the cameras where they can see at least one of the markers whose position is fixed. Click Start under Camera > Calibration > Extrinsic . When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows Extrinsic \u2611Calibrated . If it does not read the AR marker properly, try If the marker cannot be read properly . Place the cameras so that at least two of them can see your whole body at the same time. In MocapForAll, select Human motion in Settings > Calibration > Extrinsic calibration method . Select GPU_DirectML in Settings > General > Run DNN on if your hardware support it. (If you have already installed the GPU_TensorRT mode in the Appendix, you can also use it.\uff09 Select Speed in Settings > General > Capture body if your PC's performance is enough for it. (If you have already installed the Precision mode in the Appendix, you can also use it.\uff09 Click Start calibration button at the top of the window , and walk around on the floor where the cameras can see your whole body. Your motion will be captured and the positions of your joints will be used to find the cameras relative positions. After all cameras' extrinsic parameters calibrated, click the Find Ground button at the top of the window and walk around in the same way as If the marker cannot be read properly described below. This will determine the absolute position of the cameras.","title":"Get extrinsic parameters"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#if-the-marker-cannot-be-read-properly","text":"Method to place marker on the wall Since the position of the marker will be the plane with zero height of the captured motion, it is basically recommended to place the marker on the floor, but it may be difficult to read the marker on the floor depending on the placement and specification of the camera. In that case, you can hang the marker on the wall to calibrate the camera and adjust the floor level later as follows: Replace the word \"on the floor\" with \"on the wall\" and execute the calibration procedure. Select GPU_DirectML in Settings > General > Run DNN on if your hardware support it. (If you have already installed the GPU_TensorRT mode in the Appendix, you can also use it.\uff09 Select Speed in Settings > General > Capture body if your PC's performance is enough for it. (If you have already installed the Precision mode in the Appendix, you can also use it.\uff09 Click the Find Ground button at the top of the window and walk around where at least 2 cameras can see your whole body. After a while, the position of the floor level will be adjusted automatically.","title":"If the marker cannot be read properly"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#confirm-the-results","text":"If you have input the correct marker size in Prepare markers section, you can see the position of the camera in the 3D viewport. Use the WASD key and mouse drag to move the viewport on MocapForAll. Cameras may appear below the floor In this case, the calibration has failed.","title":"Confirm the results"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#save-and-load-the-results","text":"","title":"Save and load the results"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#for-each-camera","text":"After calibration is completed, save the calibration result of each camera by clicking Save under Camera > Calibration > Intrinsic and Extrinsic . The saved result can be loaded by clicking Load button.","title":"For each camera"},{"location":"how-to-capture/calibrate-cameras/execute-calibration/#for-all-cameras","text":"You can save and load the whole camera setup by clicking Save All Cameras and Load All Cameras buttons. Caution when saving all camera settings The camera selection is saved as the index inside the combo box. If you remove cameras from your PC, the order of the cameras in the combo box will change and you will not be able to load the cameras properly.","title":"For all cameras"},{"location":"how-to-capture/calibrate-cameras/prepare-markers/","text":"Prepare markers In order to calibrate a camera, it is necessary to \"find the positions of points in the camera image whose positions in the real world are known\". To do that, you need to take pictures of the \"specific images\" described below with the camera that actually you are using, so that the application can calculate the before-mentioned camera information from the pictures. In this section, we will print the \"specific images\" in preparation for camera calibration. Print marker for intrinsic parameter calibration We will use this image . If you haven't fixed the camera in the room yet, you don't need to print it out. We will use the above image by showing on the PC display. If you have already fixed the camera, print the above image in A4 size. The size does not have to be exact. Then tape it to a cardboard box to keep it flat. Print marker for extrinsic parameter calibration The image differs depending on the 4 methods to get extrinsic parameters explained before. Either way, the size does not have to be exact. ChArUco board method ArUco cluster method Diamond cluster method Human motion method We will use this image . Print this in A2 or larger . You don't have a printer which can print A2? Then, it is recommended to divide the image into two pieces, print them on two sheets of A3 paper, and tape them together. If you tape it on a cardboard like this so that it keeps in a clean flat state, you can continue to use it for a long time. \uff08The one in the photo above has been used for about 3 months, but it is still good to use.) We will use arucoMarker0.png , (same)1.png and (same)2.png in this zip . Print them in A4 or A3. A4 is enough for a room that is not very large. We will use diamondMarker0.png and others in this zip . Print them in A2 or larger in the same way as \"ChArUco board method\". There is nothing to print. Measure the marker size You need to measure the size of the actual printed image for extrinsic parameter calibration. The measured values will be used to define the scale of the captured movement. The part to measure differs depending on the 4 methods to get extrinsic parameters explained before. ChArUco board method ArUco cluster method Diamond cluster method Human motion method Put the value in Settings > Calibration > Maker size (affects to coord. scale) > ChArUco board [m] . The unit is meters. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > ArUco marker [m] . The unit is meters. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > Diamond marker [m] . The unit is meters. Measure the height of your body. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > Human hight [m] . The unit is meters.","title":"Prepare markers"},{"location":"how-to-capture/calibrate-cameras/prepare-markers/#prepare-markers","text":"In order to calibrate a camera, it is necessary to \"find the positions of points in the camera image whose positions in the real world are known\". To do that, you need to take pictures of the \"specific images\" described below with the camera that actually you are using, so that the application can calculate the before-mentioned camera information from the pictures. In this section, we will print the \"specific images\" in preparation for camera calibration.","title":"Prepare markers"},{"location":"how-to-capture/calibrate-cameras/prepare-markers/#print-marker-for-intrinsic-parameter-calibration","text":"We will use this image . If you haven't fixed the camera in the room yet, you don't need to print it out. We will use the above image by showing on the PC display. If you have already fixed the camera, print the above image in A4 size. The size does not have to be exact. Then tape it to a cardboard box to keep it flat.","title":"Print marker for intrinsic parameter calibration"},{"location":"how-to-capture/calibrate-cameras/prepare-markers/#print-marker-for-extrinsic-parameter-calibration","text":"The image differs depending on the 4 methods to get extrinsic parameters explained before. Either way, the size does not have to be exact. ChArUco board method ArUco cluster method Diamond cluster method Human motion method We will use this image . Print this in A2 or larger . You don't have a printer which can print A2? Then, it is recommended to divide the image into two pieces, print them on two sheets of A3 paper, and tape them together. If you tape it on a cardboard like this so that it keeps in a clean flat state, you can continue to use it for a long time. \uff08The one in the photo above has been used for about 3 months, but it is still good to use.) We will use arucoMarker0.png , (same)1.png and (same)2.png in this zip . Print them in A4 or A3. A4 is enough for a room that is not very large. We will use diamondMarker0.png and others in this zip . Print them in A2 or larger in the same way as \"ChArUco board method\". There is nothing to print.","title":"Print marker for extrinsic parameter calibration"},{"location":"how-to-capture/calibrate-cameras/prepare-markers/#measure-the-marker-size","text":"You need to measure the size of the actual printed image for extrinsic parameter calibration. The measured values will be used to define the scale of the captured movement. The part to measure differs depending on the 4 methods to get extrinsic parameters explained before. ChArUco board method ArUco cluster method Diamond cluster method Human motion method Put the value in Settings > Calibration > Maker size (affects to coord. scale) > ChArUco board [m] . The unit is meters. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > ArUco marker [m] . The unit is meters. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > Diamond marker [m] . The unit is meters. Measure the height of your body. Put the value in Settings > Calibration > Maker size (affects to coord. scale) > Human hight [m] . The unit is meters.","title":"Measure the marker size"},{"location":"how-to-capture/calibrate-cameras/what-is-camera-calibration/","text":"What is camera calibration? Camera calibration is the process of obtaining information about the relationship between the positions in the camera image and the positions in the real world . This page describes the concept of what you are doing with camera calibration and some tips for it. MocapForAll obtains the following 2 types of information during camera calibration: Intrinsic parameters (Characteristics of the camera itself) Extrinsic parameters (Position of the camera in the real world) What is intrinsic parameters? Intrinsic parameters are the focal length f of the lens and the position of the optical axis Cx, Cy , which describes the characteristics of the camera itself . These are unique to the camera (lens) and basically do not change over time. Therefore, once you get them correctly, you don't need to get them again. Notes on autofocus If you use a camera with autofocus, keep in mind that as the focus changes, the focal length changes, so the intrinsic parameter also changes. In our experiences, it does not cause much problems when using regular webcams or mobile phones, but if high accuracy is not obtained, you might need to disable autofocus. Mathematical representation Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera image\" and \"positions in the camera coordinate\". This is the matrix displayed on the app screen. What is extrinsic parameters? Extrinsic parameters are the position and the orientation of the camera in the real world . In theory, once the camera is completely fixed in your room, it can be treated as a fixed value. But in practice, it sometimes shifts little by little over time. So, it is recommended to obtain it again every time you start using MocapForAll . Mathematical representation Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera coordinate\" and \"positions in the world coordinate\". This is the matrix displayed on the app screen. 4 methods to get extrinsic parameters In MocapForAll, there are 4 methods to get extrinsic parameters. Please note that the preparation and execution procedures are different for each as described later. Method Accuracy Ease of preparation The size of the usable space \u3000Comment 1. Method using ChArUco board \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d \ud83d\udc4d The most accurate, but requires a little work to prepare. I think this is the easiest way in the long run. 2. Method using ArUco cluster \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This is a method with good accuracy and easy preparation. For beginners, I recommend that you try this method first. 3. Method using Diamond cluster \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc80 \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d This method allows capturing in a large space with many cameras by measuring the relative positions of multiple markers, though it takes time to prepare. 4. Method using human motion \ud83d\udc4d \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This method allows capturing in an environment where the marker cannot be printed or placed (for example, outdoors).","title":"What is camera calibration?"},{"location":"how-to-capture/calibrate-cameras/what-is-camera-calibration/#what-is-camera-calibration","text":"Camera calibration is the process of obtaining information about the relationship between the positions in the camera image and the positions in the real world . This page describes the concept of what you are doing with camera calibration and some tips for it. MocapForAll obtains the following 2 types of information during camera calibration: Intrinsic parameters (Characteristics of the camera itself) Extrinsic parameters (Position of the camera in the real world)","title":"What is camera calibration?"},{"location":"how-to-capture/calibrate-cameras/what-is-camera-calibration/#what-is-intrinsic-parameters","text":"Intrinsic parameters are the focal length f of the lens and the position of the optical axis Cx, Cy , which describes the characteristics of the camera itself . These are unique to the camera (lens) and basically do not change over time. Therefore, once you get them correctly, you don't need to get them again. Notes on autofocus If you use a camera with autofocus, keep in mind that as the focus changes, the focal length changes, so the intrinsic parameter also changes. In our experiences, it does not cause much problems when using regular webcams or mobile phones, but if high accuracy is not obtained, you might need to disable autofocus. Mathematical representation Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera image\" and \"positions in the camera coordinate\". This is the matrix displayed on the app screen.","title":"What is intrinsic parameters?"},{"location":"how-to-capture/calibrate-cameras/what-is-camera-calibration/#what-is-extrinsic-parameters","text":"Extrinsic parameters are the position and the orientation of the camera in the real world . In theory, once the camera is completely fixed in your room, it can be treated as a fixed value. But in practice, it sometimes shifts little by little over time. So, it is recommended to obtain it again every time you start using MocapForAll . Mathematical representation Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera coordinate\" and \"positions in the world coordinate\". This is the matrix displayed on the app screen.","title":"What is extrinsic parameters?"},{"location":"how-to-capture/calibrate-cameras/what-is-camera-calibration/#4-methods-to-get-extrinsic-parameters","text":"In MocapForAll, there are 4 methods to get extrinsic parameters. Please note that the preparation and execution procedures are different for each as described later. Method Accuracy Ease of preparation The size of the usable space \u3000Comment 1. Method using ChArUco board \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d \ud83d\udc4d The most accurate, but requires a little work to prepare. I think this is the easiest way in the long run. 2. Method using ArUco cluster \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This is a method with good accuracy and easy preparation. For beginners, I recommend that you try this method first. 3. Method using Diamond cluster \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc80 \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d This method allows capturing in a large space with many cameras by measuring the relative positions of multiple markers, though it takes time to prepare. 4. Method using human motion \ud83d\udc4d \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This method allows capturing in an environment where the marker cannot be printed or placed (for example, outdoors).","title":"4 methods to get extrinsic parameters"},{"location":"how-to-export/to-bvh-files/","text":"Export motion to BVH files Click Stop capture . Drag and drop the VRM file into the MocapForAll window to load a VRM model. Turn on Settings > Data export > Record to BVH file Settings > General > Character will be automatically set to VRM runtime load Click Start capture to start saving the data in BVH format. Click Stop capture to generate the final BVH file. In the case of the free trial version, a BVH file will be automatically generated after 300 frames have passed, and data saving will be stopped. Bone structure is based on VRM After retargeting the animation to the VRM model on MocapForAll, the animation is exported in BVH format. Therefore, when exporting data in BVH format, it automatically switches to the mode that uses VRM. If you do not own VRM models Please select VRM runtime load in Settings > General > Character without actually loading a model. A dummy VRM model runs in background. The unit of length is meters The unit of bone length in the generated BVH file is meters. Set the appropriate unit in the application which loads the BVH file.","title":"Export to BVH files"},{"location":"how-to-export/to-bvh-files/#export-motion-to-bvh-files","text":"Click Stop capture . Drag and drop the VRM file into the MocapForAll window to load a VRM model. Turn on Settings > Data export > Record to BVH file Settings > General > Character will be automatically set to VRM runtime load Click Start capture to start saving the data in BVH format. Click Stop capture to generate the final BVH file. In the case of the free trial version, a BVH file will be automatically generated after 300 frames have passed, and data saving will be stopped. Bone structure is based on VRM After retargeting the animation to the VRM model on MocapForAll, the animation is exported in BVH format. Therefore, when exporting data in BVH format, it automatically switches to the mode that uses VRM. If you do not own VRM models Please select VRM runtime load in Settings > General > Character without actually loading a model. A dummy VRM model runs in background. The unit of length is meters The unit of bone length in the generated BVH file is meters. Set the appropriate unit in the application which loads the BVH file.","title":"Export motion to BVH files"},{"location":"how-to-export/to-fbx-files/","text":"Export motion to FBX files Turn on Settings > Data export > Record to FBX file Settings > General > Character will be automatically set to UE Mannequin Click Start capture to start saving the data in FBX format. Click Stop capture to generate the final FBX file. In the case of the free trial version, a FBX file will be automatically generated after 300 frames have passed, and data saving will be stopped. Bone structure is based on UE Mannequin After retargeting the animation to the UE Mannequin model on MocapForAll, the animation is exported in FBX format. Therefore, when exporting data in FBX format, it automatically switches to the UE Mannequin model. Animation is baked at fixed framerate of 25fps After baking the animation at fixed framerate of 25fps on MocapForAll, the animation is exported in FBX format. No facial expression exported Facial expressions are not supported to export.","title":"Export to FBX files"},{"location":"how-to-export/to-fbx-files/#export-motion-to-fbx-files","text":"Turn on Settings > Data export > Record to FBX file Settings > General > Character will be automatically set to UE Mannequin Click Start capture to start saving the data in FBX format. Click Stop capture to generate the final FBX file. In the case of the free trial version, a FBX file will be automatically generated after 300 frames have passed, and data saving will be stopped. Bone structure is based on UE Mannequin After retargeting the animation to the UE Mannequin model on MocapForAll, the animation is exported in FBX format. Therefore, when exporting data in FBX format, it automatically switches to the UE Mannequin model. Animation is baked at fixed framerate of 25fps After baking the animation at fixed framerate of 25fps on MocapForAll, the animation is exported in FBX format. No facial expression exported Facial expressions are not supported to export.","title":"Export motion to FBX files"},{"location":"how-to-export/to-game-engines/","text":"Export motion to game engines By receiver Plugins Plugins for Unreal Engine 4, Unreal Engine 5, and Unity are available here . Does it use VMT? These plugin uses VMT protocol for the communication, though they do not need Virtual Motion Tracker itself and SteamVR. How to use Unreal Engine plugin Manual of the receiver plugin for UE4 Manual of the receiver plugin for UE5 How to use Unity plugin Readme of the receiver plugin for Unity Readme of the receiver plugin for Unity, using shared memory By VMC4UE or EVMC4U As described in To vmc marionette section, set MocapForAll settings. VMC4UE As described in official wiki page , set up UE4. Click Play on UE4 editor. (You do not need last \"VirtualMotionCapture \u306e\u4f7f\u3044\u65b9\" section.) EVMC4U Execute \"0. Open Unity project\" to \"4. Let's play\" of ExternalReceiverPack setup in official wiki page","title":"Export to game engines"},{"location":"how-to-export/to-game-engines/#export-motion-to-game-engines","text":"","title":"Export motion to game engines"},{"location":"how-to-export/to-game-engines/#by-receiver-plugins","text":"Plugins for Unreal Engine 4, Unreal Engine 5, and Unity are available here . Does it use VMT? These plugin uses VMT protocol for the communication, though they do not need Virtual Motion Tracker itself and SteamVR.","title":"By receiver Plugins"},{"location":"how-to-export/to-game-engines/#how-to-use-unreal-engine-plugin","text":"Manual of the receiver plugin for UE4 Manual of the receiver plugin for UE5","title":"How to use Unreal Engine plugin"},{"location":"how-to-export/to-game-engines/#how-to-use-unity-plugin","text":"Readme of the receiver plugin for Unity Readme of the receiver plugin for Unity, using shared memory","title":"How to use Unity plugin"},{"location":"how-to-export/to-game-engines/#by-vmc4ue-or-evmc4u","text":"As described in To vmc marionette section, set MocapForAll settings. VMC4UE As described in official wiki page , set up UE4. Click Play on UE4 editor. (You do not need last \"VirtualMotionCapture \u306e\u4f7f\u3044\u65b9\" section.) EVMC4U Execute \"0. Open Unity project\" to \"4. Let's play\" of ExternalReceiverPack setup in official wiki page","title":"By VMC4UE or EVMC4U"},{"location":"how-to-export/to-shared-memory/","text":"Export motion to shared memory You can export bone transform and facial expression raw data to shared memory. Settings to send data Turn on Settings > Data export > Shared Memory > Export to shared memory Change the name of destination shared memory Data will be written to the shared memory of the name specified in Settings > Data export > Shared Memory > Names of shared memories Format of output data The first 1 byte is a counter that is incremented by 1 each time the data is updated (after 255, it returns to 0). The next 4 bytes are the int type value which describes the length of the remaining data in bytes. The rest are array of 4-byte float values. Body Hand The orientation and position of 15 bones are exported in the similar format as the Body data. The arrangement of bones is as follows. Face Only positions (no rotations) are exported for 468 face landmarks. The position is represented by a three-dimensional value with the XY coordinates in pixels in the camera image cropped to the area of the face and the Z coordinate of the same scale in the depth direction. Enlarge the image below to see where each landmark corresponds to your face. Eye In the same format as the Face data, the positions of 5 points for the iris and 71 points for the eyelid are exported. Sample project A sample project to read the transform data from shared memory with Unity and apply it to GameObjects is available here .","title":"Export to shared memory"},{"location":"how-to-export/to-shared-memory/#export-motion-to-shared-memory","text":"You can export bone transform and facial expression raw data to shared memory.","title":"Export motion to shared memory"},{"location":"how-to-export/to-shared-memory/#settings-to-send-data","text":"Turn on Settings > Data export > Shared Memory > Export to shared memory Change the name of destination shared memory Data will be written to the shared memory of the name specified in Settings > Data export > Shared Memory > Names of shared memories","title":"Settings to send data"},{"location":"how-to-export/to-shared-memory/#format-of-output-data","text":"The first 1 byte is a counter that is incremented by 1 each time the data is updated (after 255, it returns to 0). The next 4 bytes are the int type value which describes the length of the remaining data in bytes. The rest are array of 4-byte float values.","title":"Format of output data"},{"location":"how-to-export/to-shared-memory/#body","text":"","title":"Body"},{"location":"how-to-export/to-shared-memory/#hand","text":"The orientation and position of 15 bones are exported in the similar format as the Body data. The arrangement of bones is as follows.","title":"Hand"},{"location":"how-to-export/to-shared-memory/#face","text":"Only positions (no rotations) are exported for 468 face landmarks. The position is represented by a three-dimensional value with the XY coordinates in pixels in the camera image cropped to the area of the face and the Z coordinate of the same scale in the depth direction. Enlarge the image below to see where each landmark corresponds to your face.","title":"Face"},{"location":"how-to-export/to-shared-memory/#eye","text":"In the same format as the Face data, the positions of 5 points for the iris and 71 points for the eyelid are exported.","title":"Eye"},{"location":"how-to-export/to-shared-memory/#sample-project","text":"A sample project to read the transform data from shared memory with Unity and apply it to GameObjects is available here .","title":"Sample project"},{"location":"how-to-export/to-steamvr/","text":"Export motion to SteamVR You can use captured motion as virtual trackers in SteamVR by using \" Virtual Motion Tracker \"(VMT) created by gpsnmeajp. Warning Please note that VMT is a separated program from MocapForAll. DO NOT contact the author of VMT for any issues you encountered when using MocapForAll and VMT together. Install VMT Download from here . Follow the setup procedure of VMT. Which version of VMT should I use? Use the special version of VMT modifed for MocapForAll to align automatically the coordinates of SteamVR and MocapForAll . If you align them manually, you do not need to use that version. Use vmt_011 or above. vmt_013c is recommended. If trackers don't work for your app Some application requires settings of controller binding . If you have problems with VMT First, see trouble shooting page . Settings to send data Set Settings > Data export > Destination IP address for VMT and VMC as follows: If the destination is the same PC: 127.0.0.1 If the destination is another PC: IP address of the destination PC You can check it by typing ipconfig /all in the command prompt By running MocapForAll and SteamVR on different PCs, you can offload the mocap calculation and play VR games comfortably. Turn on Settings > Data export > VMT protocol > Send tracking points Set the port of Settings> Data export> VMT protocol> Send tracking points to 39570 . Turn on the required parts under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent . For example, if you want to use it in VRChat, turn on Pelvis and Feet You can adjust the offset of the virtual tracker position by clicking the > on the left side of each. Advanced setting: As relative position to HMD For the following settings, please decide on/off by your preference. We recommend that you try turning on first, and then try turning off if you are really serious about the motion. Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD When turned off , the actual tracking positions will be used as the positions of the virtual trackers. The delay of the movements of the virtual trackers with respect to the head-mounted display will be visible as it is . (For example, when you start walking, it looks like only the head moves first and the rest moves later.) You need to adjust the scale and the positions properly in Align coordinates of SteamVR and MocapForAll section described below. (If the scale is not set properly, the horizontal position of the virtual trackers relative to the HMD shifts as you move in the room.) When turned on , the relative positions of your head and the rest parts will be transformed to the relative positions of HMD and the virtual trackers. The delay of the movements of the virtual trackers with respect to the HMD will be less noticeable . (For example, when you start walking, it looks like the rest part slides along with the head.) You do not need to input precise number to the scale, and do not need to input the positions in Align coordinates of SteamVR and MocapForAll section described below. Align coordinates of SteamVR and MocapForAll The coordinate of the HMD tracked by SteamVR and the coordinate of the body positions tracked by MocapForAll do not match as they are. You need to align them manually or automatically. Align automatically Align manually Prerequisite As described in Install VMT section, you need to install the modified version of Virtual Motion Tracker for MocapForAll . Close application which uses port 39571 including VMT Manager. Turn on Settings > Data export > VMT protocol > Send tracking points Put on the head-mounted display and keep SteamVR running. Stand in a position where HMD is tracked by SteamVR and your body is tracked by MocapForAll. In MocapForAll, click Align coord. To VR at the top of the window. Walk around. After a while, the value of Settings > Coordinates will be updated automatically, and the coordinates of SteamVR and MocapForAll will be aligned. Preparation Click Start capture at the top of the MocapForAll window to start motion capture Put on your head-mounted display, quit apps running on SteamVR, including SteamVR Home, and make your virtual tracker visible on the SteamVR default screen. Adjust the scale In MocapForAll, Turn on Head under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr off Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Set the values of Settings > Coordinates > Scales > Upper body and Lower body so that the height of your eye level matches the height of the Head virtual tracker. Upper body and Lower body scales should basically have the same value. Adjust the orientation In MocapForAll, Turn on Feet under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr on Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Move your foot back and forth and set the value of Settings > Coordinates > Orientation offset so that the direction of the movement of Foot virtual tracker matches the direction of the actual movement of your foot. Adjust the position In MocapForAll, Turn on Feet under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr off Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Set the value of Settings > Coordinates > Position offset so that the position of your feet matches the position of the virtual tracker of Feet .","title":"Export to SteamVR"},{"location":"how-to-export/to-steamvr/#export-motion-to-steamvr","text":"You can use captured motion as virtual trackers in SteamVR by using \" Virtual Motion Tracker \"(VMT) created by gpsnmeajp. Warning Please note that VMT is a separated program from MocapForAll. DO NOT contact the author of VMT for any issues you encountered when using MocapForAll and VMT together.","title":"Export motion to SteamVR"},{"location":"how-to-export/to-steamvr/#install-vmt","text":"Download from here . Follow the setup procedure of VMT. Which version of VMT should I use? Use the special version of VMT modifed for MocapForAll to align automatically the coordinates of SteamVR and MocapForAll . If you align them manually, you do not need to use that version. Use vmt_011 or above. vmt_013c is recommended. If trackers don't work for your app Some application requires settings of controller binding . If you have problems with VMT First, see trouble shooting page .","title":"Install VMT"},{"location":"how-to-export/to-steamvr/#settings-to-send-data","text":"Set Settings > Data export > Destination IP address for VMT and VMC as follows: If the destination is the same PC: 127.0.0.1 If the destination is another PC: IP address of the destination PC You can check it by typing ipconfig /all in the command prompt By running MocapForAll and SteamVR on different PCs, you can offload the mocap calculation and play VR games comfortably. Turn on Settings > Data export > VMT protocol > Send tracking points Set the port of Settings> Data export> VMT protocol> Send tracking points to 39570 . Turn on the required parts under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent . For example, if you want to use it in VRChat, turn on Pelvis and Feet You can adjust the offset of the virtual tracker position by clicking the > on the left side of each. Advanced setting: As relative position to HMD For the following settings, please decide on/off by your preference. We recommend that you try turning on first, and then try turning off if you are really serious about the motion. Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD When turned off , the actual tracking positions will be used as the positions of the virtual trackers. The delay of the movements of the virtual trackers with respect to the head-mounted display will be visible as it is . (For example, when you start walking, it looks like only the head moves first and the rest moves later.) You need to adjust the scale and the positions properly in Align coordinates of SteamVR and MocapForAll section described below. (If the scale is not set properly, the horizontal position of the virtual trackers relative to the HMD shifts as you move in the room.) When turned on , the relative positions of your head and the rest parts will be transformed to the relative positions of HMD and the virtual trackers. The delay of the movements of the virtual trackers with respect to the HMD will be less noticeable . (For example, when you start walking, it looks like the rest part slides along with the head.) You do not need to input precise number to the scale, and do not need to input the positions in Align coordinates of SteamVR and MocapForAll section described below.","title":"Settings to send data"},{"location":"how-to-export/to-steamvr/#align-coordinates-of-steamvr-and-mocapforall","text":"The coordinate of the HMD tracked by SteamVR and the coordinate of the body positions tracked by MocapForAll do not match as they are. You need to align them manually or automatically. Align automatically Align manually Prerequisite As described in Install VMT section, you need to install the modified version of Virtual Motion Tracker for MocapForAll . Close application which uses port 39571 including VMT Manager. Turn on Settings > Data export > VMT protocol > Send tracking points Put on the head-mounted display and keep SteamVR running. Stand in a position where HMD is tracked by SteamVR and your body is tracked by MocapForAll. In MocapForAll, click Align coord. To VR at the top of the window. Walk around. After a while, the value of Settings > Coordinates will be updated automatically, and the coordinates of SteamVR and MocapForAll will be aligned. Preparation Click Start capture at the top of the MocapForAll window to start motion capture Put on your head-mounted display, quit apps running on SteamVR, including SteamVR Home, and make your virtual tracker visible on the SteamVR default screen. Adjust the scale In MocapForAll, Turn on Head under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr off Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Set the values of Settings > Coordinates > Scales > Upper body and Lower body so that the height of your eye level matches the height of the Head virtual tracker. Upper body and Lower body scales should basically have the same value. Adjust the orientation In MocapForAll, Turn on Feet under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr on Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Move your foot back and forth and set the value of Settings > Coordinates > Orientation offset so that the direction of the movement of Foot virtual tracker matches the direction of the actual movement of your foot. Adjust the position In MocapForAll, Turn on Feet under Settings > Data export > VMT protocol > Send tracking points > Tracking points to be sent Tunr off Settings > Data export > VMT protocol > Send tracking points > As relative position to HMD Set the value of Settings > Coordinates > Position offset so that the position of your feet matches the position of the virtual tracker of Feet .","title":"Align coordinates of SteamVR and MocapForAll"},{"location":"how-to-export/to-vmc-marionette/","text":"Export motion to apps which accept VMC protocol You can send the captured motion to various applications via VMC protocol. The following are confirmed to work: Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending bones and facial expression morphs to EVMC4U Sending bones and facial expression morphs to VMC4UE Sending bones to VMC4B Sending tracker to VirtualMotionCapture Do I need VMC? \"VMC\" itself is not required for communication between applications which send/accept \"VMC protocol\". (VirtualMotionCapture is one example of an application that supports communication via the VMC protocol.) Load VRM models Stop capturing Drag and drop the VRM file into the MocapForAll window Notice Load the same VRM model with the destination app. Direct or Indirect mode? Try Direct mode first. Indirect mode is only for models which do not work with Direct mode properly. Settings to send data Set Settings > Data export > Destination IP address for VMT and VMC as follows: If the destination is the same PC: 127.0.0.1 If the destination is another PC: IP address of the destination PC You can check it by typing ipconfig /all in the command prompt Turn on Settings > Data export > VMC protocol > Send bones Set the port of Settings> Data export> VMC protocol> Send tracking points according to the destination port. Advanced setting: As trackers If you want to send as trackers (/VMC/Ext/Tra/Pos), not bones (/VMC/Ext/Bone/Pos), set the following: (For example, this setting is required to send data to VirtualMotionCapture.) Turn on Settings > Data export > VMC protocol > Send bones > As trackers (/VMC/Ext/Tra/Pos) Settings to receive and send facial expressions To receive facial expression morph data, set as follows: Turn on Settings > Data export > VMC protocol > Receive facial morphs Set the port of Settings > Data export > VMC protocol > Receive facial morphs according to the destination port specified by the source app. Set Settings > Data export > VMC protocol > Receive facial morphs > From other device as follows: If the source app is running on the same PC: Off If the source app is running on other device: On The received facial expression morph data will be sent as it is if Settings to send data is set.","title":"Export to VMC marionette"},{"location":"how-to-export/to-vmc-marionette/#export-motion-to-apps-which-accept-vmc-protocol","text":"You can send the captured motion to various applications via VMC protocol. The following are confirmed to work: Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending bones and facial expression morphs to EVMC4U Sending bones and facial expression morphs to VMC4UE Sending bones to VMC4B Sending tracker to VirtualMotionCapture Do I need VMC? \"VMC\" itself is not required for communication between applications which send/accept \"VMC protocol\". (VirtualMotionCapture is one example of an application that supports communication via the VMC protocol.)","title":"Export motion to apps which accept VMC protocol"},{"location":"how-to-export/to-vmc-marionette/#load-vrm-models","text":"Stop capturing Drag and drop the VRM file into the MocapForAll window Notice Load the same VRM model with the destination app. Direct or Indirect mode? Try Direct mode first. Indirect mode is only for models which do not work with Direct mode properly.","title":"Load VRM models"},{"location":"how-to-export/to-vmc-marionette/#settings-to-send-data","text":"Set Settings > Data export > Destination IP address for VMT and VMC as follows: If the destination is the same PC: 127.0.0.1 If the destination is another PC: IP address of the destination PC You can check it by typing ipconfig /all in the command prompt Turn on Settings > Data export > VMC protocol > Send bones Set the port of Settings> Data export> VMC protocol> Send tracking points according to the destination port. Advanced setting: As trackers If you want to send as trackers (/VMC/Ext/Tra/Pos), not bones (/VMC/Ext/Bone/Pos), set the following: (For example, this setting is required to send data to VirtualMotionCapture.) Turn on Settings > Data export > VMC protocol > Send bones > As trackers (/VMC/Ext/Tra/Pos)","title":"Settings to send data"},{"location":"how-to-export/to-vmc-marionette/#settings-to-receive-and-send-facial-expressions","text":"To receive facial expression morph data, set as follows: Turn on Settings > Data export > VMC protocol > Receive facial morphs Set the port of Settings > Data export > VMC protocol > Receive facial morphs according to the destination port specified by the source app. Set Settings > Data export > VMC protocol > Receive facial morphs > From other device as follows: If the source app is running on the same PC: Off If the source app is running on other device: On The received facial expression morph data will be sent as it is if Settings to send data is set.","title":"Settings to receive and send facial expressions"},{"location":"how-to-export/to-vrchat-osc/","text":"Export to VRChat You can export joint positions as OSC Tracker for full-body tracking in VRChat. OSC Tracker is a beta feature As of February 1, 2023, OSC Tracker is not an officially released feature of VRChat and is not guaranteed to work properly. Please see the VRChat documentation for more information. Reference: OSC Overview Basic settings Turn on Settings > Data export > VRChat OSC (\u03b2) and set Dest. IP address as follows: If the destination is the same PC: 127.0.0.1 If the destination is another device (for example, Quest 2): IP address of the destination PC Alignment on VRChat As described in VRChat's documentation here , you can press Auto-center OSC Trackers to automatically align the tracking spaces of VRChat and MocapForAll. Note that this button only appears when VRChat is receiving data via OSC. Select joints By turning on/off each joint under Settings > Data export > VRChat OSC (\u03b2) , you can select the joints to be sent. Input Tracker index for each joint. Move relative to the head If \"Tracker index\" is set to \"0\", the position of that joint is sent as \"/tracking/trackers/head\". (By default, index=0 is set for the head joint.) This is used when you want to move each joint relative to the head-mounted display. With this feature, the relative position of each joint to the head in MocapForAll is converted to the relative position of each tracker to the head-mounted display in VRChat. The delay of the movements of the virtual trackers with respect to the HMD will be less noticeable . (For example, when you start walking, it looks like the rest part slides along with the head.) If this feature is not used, the actual tracking position of each joint is used as the virtual tracker position. The delay of the movements of the virtual trackers with respect to the head-mounted display will be visible as it is . (For example, when you start walking, it looks like only the head moves first and the rest moves later.) Note that this function will also automatically align the orientation of the VRChat and MocapForAll tracking coordinates based on the orientation of the head. For this to work properly, you must specify Z=90 as the orientation offset for MocapForAll's head joint. This value is already specified by default. See VRChat's documentation here for more information.","title":"Export to VRChat"},{"location":"how-to-export/to-vrchat-osc/#export-to-vrchat","text":"You can export joint positions as OSC Tracker for full-body tracking in VRChat. OSC Tracker is a beta feature As of February 1, 2023, OSC Tracker is not an officially released feature of VRChat and is not guaranteed to work properly. Please see the VRChat documentation for more information. Reference: OSC Overview","title":"Export to VRChat"},{"location":"how-to-export/to-vrchat-osc/#basic-settings","text":"Turn on Settings > Data export > VRChat OSC (\u03b2) and set Dest. IP address as follows: If the destination is the same PC: 127.0.0.1 If the destination is another device (for example, Quest 2): IP address of the destination PC","title":"Basic settings"},{"location":"how-to-export/to-vrchat-osc/#alignment-on-vrchat","text":"As described in VRChat's documentation here , you can press Auto-center OSC Trackers to automatically align the tracking spaces of VRChat and MocapForAll. Note that this button only appears when VRChat is receiving data via OSC.","title":"Alignment on VRChat"},{"location":"how-to-export/to-vrchat-osc/#select-joints","text":"By turning on/off each joint under Settings > Data export > VRChat OSC (\u03b2) , you can select the joints to be sent. Input Tracker index for each joint. Move relative to the head If \"Tracker index\" is set to \"0\", the position of that joint is sent as \"/tracking/trackers/head\". (By default, index=0 is set for the head joint.) This is used when you want to move each joint relative to the head-mounted display. With this feature, the relative position of each joint to the head in MocapForAll is converted to the relative position of each tracker to the head-mounted display in VRChat. The delay of the movements of the virtual trackers with respect to the HMD will be less noticeable . (For example, when you start walking, it looks like the rest part slides along with the head.) If this feature is not used, the actual tracking position of each joint is used as the virtual tracker position. The delay of the movements of the virtual trackers with respect to the head-mounted display will be visible as it is . (For example, when you start walking, it looks like only the head moves first and the rest moves later.) Note that this function will also automatically align the orientation of the VRChat and MocapForAll tracking coordinates based on the orientation of the head. For this to work properly, you must specify Z=90 as the orientation offset for MocapForAll's head joint. This value is already specified by default. See VRChat's documentation here for more information.","title":"Select joints"},{"location":"how-to-install/install-mocapforall/","text":"How to install MocapForAll You can install from Steam or BOOTH. From Steam From BOOTH Demo and Paid versions are available. Before purchase, Read and accept the terms and conditions . Try Demo version to check if the app works in your environment properly. Go to Steam store Demo version limitation Demo version has limited functionality only for data export. Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds. Maximum frames in a BVH file is limited to 300. Appendix All Appendix are installed automatically. Appendix features Appendix is originally the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. If you want to use GPU acceleration by TensorRT, you need to manually install CUDA, cuDNN, and TensorRT . Download Free trial version Download from BOOTH Free trial version limitation Free trial version has limited functionality only for data export. Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds. Maximum frames in a BVH file is limited to 300. Paid version Before purchase, Read and accept the terms and conditions . It is required to try free trial version to check if the app works in your environment properly. If you agree, you can get your purchase password from our HP and proceed to the purchase page at BOOTH. Our HP Install You can install manually or by using network installer. Manual install By Network Installer Main files Download and unzip \"MocapForAll_Full_vN.N.N.zip\" Execute MocapForAll.exe in \"MocapForAll_Full_vN.N.N\" folder If the \"UE4 Prerequisites\" installation screen is displayed, install it. Appendix (Optional) If you do not need the functions in Appendix, you can skip this step. Appendix features Appendix is the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. Download and unzip \"AppendixN_xxxxx_yyyyy.zip\". Overwrite \"MocapForAll_Full_vN.N.N\\MocapForAll\" with \"AppendixN_xxxxx_yyyyy\\MocapForAll\". If you install \"Appendix4_TensorRT_mode\", follow the installation guide of TensorRT . Download and unzip \"Network_Installer_-_MocapForAll_Full_vN.N.N.zip\". Execute \"Network_Installer_-_MocapForAll_Full_vN.N.N.exe\". Select the Appendix you use. Appendix features Appendix is the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. To use \"Appendix4_TensorRT_mode\", see Installation of TensorRT and install the required software. Run MocapForAll from the Start menu or MocapForAll.exe in the installation path. If the \"UE4 Prerequisites\" installation screen is displayed, install it. Update You can update manually or by using network installer. Manual update By Network Installer Download and unzip \"MocapForAll_Full_vN.N.N.zip\". Overwrite the old version of \"MocapForAll_Full_vM.M.M\" folder with new \"MocapForAll_Full_vN.N.N\" folder. Same as installation. Reduce donwload size If you wan to reduce the data size to download, select only \"Main Files\" in \"Select Components\" screen without selecting \"Appendix\" and execute installation. Since the installer does not delete files, the previous Appendix remains . After that, Appendix will be treated as not installed on the \"Select Components\" screen of the installer, but this cause no problem.","title":"Install MocapForAll"},{"location":"how-to-install/install-mocapforall/#how-to-install-mocapforall","text":"You can install from Steam or BOOTH. From Steam From BOOTH Demo and Paid versions are available. Before purchase, Read and accept the terms and conditions . Try Demo version to check if the app works in your environment properly. Go to Steam store Demo version limitation Demo version has limited functionality only for data export. Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds. Maximum frames in a BVH file is limited to 300. Appendix All Appendix are installed automatically. Appendix features Appendix is originally the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. If you want to use GPU acceleration by TensorRT, you need to manually install CUDA, cuDNN, and TensorRT .","title":"How to install MocapForAll"},{"location":"how-to-install/install-mocapforall/#download","text":"","title":"Download"},{"location":"how-to-install/install-mocapforall/#free-trial-version","text":"Download from BOOTH Free trial version limitation Free trial version has limited functionality only for data export. Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds. Maximum frames in a BVH file is limited to 300.","title":"Free trial version"},{"location":"how-to-install/install-mocapforall/#paid-version","text":"Before purchase, Read and accept the terms and conditions . It is required to try free trial version to check if the app works in your environment properly. If you agree, you can get your purchase password from our HP and proceed to the purchase page at BOOTH. Our HP","title":"Paid version"},{"location":"how-to-install/install-mocapforall/#install","text":"You can install manually or by using network installer. Manual install By Network Installer","title":"Install"},{"location":"how-to-install/install-mocapforall/#main-files","text":"Download and unzip \"MocapForAll_Full_vN.N.N.zip\" Execute MocapForAll.exe in \"MocapForAll_Full_vN.N.N\" folder If the \"UE4 Prerequisites\" installation screen is displayed, install it.","title":"Main files"},{"location":"how-to-install/install-mocapforall/#appendix-optional","text":"If you do not need the functions in Appendix, you can skip this step. Appendix features Appendix is the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. Download and unzip \"AppendixN_xxxxx_yyyyy.zip\". Overwrite \"MocapForAll_Full_vN.N.N\\MocapForAll\" with \"AppendixN_xxxxx_yyyyy\\MocapForAll\". If you install \"Appendix4_TensorRT_mode\", follow the installation guide of TensorRT . Download and unzip \"Network_Installer_-_MocapForAll_Full_vN.N.N.zip\". Execute \"Network_Installer_-_MocapForAll_Full_vN.N.N.exe\". Select the Appendix you use. Appendix features Appendix is the feature separated from the main program. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. To use \"Appendix4_TensorRT_mode\", see Installation of TensorRT and install the required software. Run MocapForAll from the Start menu or MocapForAll.exe in the installation path. If the \"UE4 Prerequisites\" installation screen is displayed, install it.","title":"Appendix (Optional)"},{"location":"how-to-install/install-mocapforall/#update","text":"You can update manually or by using network installer. Manual update By Network Installer Download and unzip \"MocapForAll_Full_vN.N.N.zip\". Overwrite the old version of \"MocapForAll_Full_vM.M.M\" folder with new \"MocapForAll_Full_vN.N.N\" folder. Same as installation. Reduce donwload size If you wan to reduce the data size to download, select only \"Main Files\" in \"Select Components\" screen without selecting \"Appendix\" and execute installation. Since the installer does not delete files, the previous Appendix remains . After that, Appendix will be treated as not installed on the \"Select Components\" screen of the installer, but this cause no problem.","title":"Update"},{"location":"how-to-install/install-tensorrt/","text":"Install TensorRT You can skip this page If you are not going to use TensorRT, you can skip this page. This section is only for enthusiasts who want to improve performance even by 1ms/frame. (Using GTX1080Ti in our dev env, TensorRT has better performance than DirectML by 1ms/frame/camera in Precision mode, for example.) Requires an Nvidia GPU that supports CUDA, cuDNN, and TensorRT. Please note that the versions of cuDNN and TensorRT are different for RTX30** series and others as shown below. Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 v8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 Acknowledgements The versions for RTX30** series were provided by \u6f06\u539f \u938c\u8db3 san. Thank you! For RTX2080Ti users If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations. For RTX30** series users For the RTX30** series, we only tested RTX3060Ti and RTX3070. Others are not tested. From now on, we will only explain the case of other than RTX30** series . If you are using RTX30** series, please read the versions as appropriate. Install CUDA 11.0.3 Please note that the installer may fail to install the NVIDIA driver. In that case, please install the latest NVIDIA driver manually. Download and unzip cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 Overwrite the following folders C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\lib with unzipped cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\bin cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\include cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\lib Add an environment variable \"CUDNN_PATH\", and set its value to \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\". Download and unzip TensorRT7.1.3.4 Note that a Nvidia account is required. Add the path of lib folder in that to the environment variable \"PATH\", for example \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT-7.1.3.4\\lib\" Add an environment variable \"ORT_TENSORRT_ENGINE_CACHE_ENABLE\" and set its value to \"1\". Add an environment variable \"ORT_TENSORRT_CACHE_PATH\" and set its value to any path where you want to save the cache files, for example \"C:\\temp\". (For other options of TensorRT, see the official documentation ) (Your environment variables will be something like this) \u200b (Your environment variable \"Path\" will be something like this)","title":"Install TensorRT"},{"location":"how-to-install/install-tensorrt/#install-tensorrt","text":"You can skip this page If you are not going to use TensorRT, you can skip this page. This section is only for enthusiasts who want to improve performance even by 1ms/frame. (Using GTX1080Ti in our dev env, TensorRT has better performance than DirectML by 1ms/frame/camera in Precision mode, for example.) Requires an Nvidia GPU that supports CUDA, cuDNN, and TensorRT. Please note that the versions of cuDNN and TensorRT are different for RTX30** series and others as shown below. Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 v8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 Acknowledgements The versions for RTX30** series were provided by \u6f06\u539f \u938c\u8db3 san. Thank you! For RTX2080Ti users If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations. For RTX30** series users For the RTX30** series, we only tested RTX3060Ti and RTX3070. Others are not tested. From now on, we will only explain the case of other than RTX30** series . If you are using RTX30** series, please read the versions as appropriate. Install CUDA 11.0.3 Please note that the installer may fail to install the NVIDIA driver. In that case, please install the latest NVIDIA driver manually. Download and unzip cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 Overwrite the following folders C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\lib with unzipped cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\bin cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\include cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\lib Add an environment variable \"CUDNN_PATH\", and set its value to \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\". Download and unzip TensorRT7.1.3.4 Note that a Nvidia account is required. Add the path of lib folder in that to the environment variable \"PATH\", for example \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT-7.1.3.4\\lib\" Add an environment variable \"ORT_TENSORRT_ENGINE_CACHE_ENABLE\" and set its value to \"1\". Add an environment variable \"ORT_TENSORRT_CACHE_PATH\" and set its value to any path where you want to save the cache files, for example \"C:\\temp\". (For other options of TensorRT, see the official documentation ) (Your environment variables will be something like this) \u200b (Your environment variable \"Path\" will be something like this)","title":"Install TensorRT"},{"location":"other-settings/","text":"Others Lock-on to the character Turn on Settings > General > Look at character to make the view follow the movement of the character. Display the tracking positions Turn on Settings > General > Draw tracking points to show the yellowish cubes to display the tracking positions. Language You can change the language from the earth icon on the upper right corner of the window. Currently, Japanese, English, and French are supported. Reset all the settings Delete C:\\Users\\\\[User name]\\AppData\\Local\\MocapForAll to reset all the settings If MocapForAll does not launch for some reason, resetting the settings may solve the problem. Execute some actions at startup by command line arguments From v1.13, you can execute a specific operation at startup by specifying the following phrase in the arguments. StartCapture : Execute the same process as Start Capture button is pressed immediately after startup. LoadAllCameras : Execute the same process as Load All Cameras button is pressed immediately after startup. Example: Apply motion only to the upper body With the following settings, the motion will be applied only to the upper body of the character. - Turn off Settings > General > Capture body > Apply to lower body . The lower body stops at the position of the last frame, so adjust the relative position of the upper and lower body with Settings> Coordinates if necessary.","title":"Others"},{"location":"other-settings/#others","text":"","title":"Others"},{"location":"other-settings/#lock-on-to-the-character","text":"Turn on Settings > General > Look at character to make the view follow the movement of the character.","title":"Lock-on to the character"},{"location":"other-settings/#display-the-tracking-positions","text":"Turn on Settings > General > Draw tracking points to show the yellowish cubes to display the tracking positions.","title":"Display the tracking positions"},{"location":"other-settings/#language","text":"You can change the language from the earth icon on the upper right corner of the window. Currently, Japanese, English, and French are supported.","title":"Language"},{"location":"other-settings/#reset-all-the-settings","text":"Delete C:\\Users\\\\[User name]\\AppData\\Local\\MocapForAll to reset all the settings If MocapForAll does not launch for some reason, resetting the settings may solve the problem.","title":"Reset all the settings"},{"location":"other-settings/#execute-some-actions-at-startup-by-command-line-arguments","text":"From v1.13, you can execute a specific operation at startup by specifying the following phrase in the arguments. StartCapture : Execute the same process as Start Capture button is pressed immediately after startup. LoadAllCameras : Execute the same process as Load All Cameras button is pressed immediately after startup. Example:","title":"Execute some actions at startup by command line arguments"},{"location":"other-settings/#apply-motion-only-to-the-upper-body","text":"With the following settings, the motion will be applied only to the upper body of the character. - Turn off Settings > General > Capture body > Apply to lower body . The lower body stops at the position of the last frame, so adjust the relative position of the upper and lower body with Settings> Coordinates if necessary.","title":"Apply motion only to the upper body"},{"location":"other-settings/capture-from-recorded-video/","text":"Capture from recorded video From v1.12, you can capture motion using recorded videos. How to use Select Recorded video from the \u25bc next to Add camera at the top of the MocapForAll window. Press the ... button and select the video file. Do not use multi-byte characters If the file path contains multi-byte characters (e.g. Japanese hiragana), the video file may fail to open. After that, the usage is basically the same as a normal webcam. Playback positions The video will automatically loop. The playback position of the video will return to the beginning of the video when you press Start Capture . Tips It is recommended to separate the video files for intrinsic parameter calibration, extrinsic parameter calibration, and actual motion capture. There is no function to analyze the videos and synchronize the movements between them, so it is necessary to edit the videos in advance so that the movements will be synchronized when they are played at the same time from the beginning. When recording videos, it is recommended to clarify the starting point with a clapperboard or the sound of clapping your hands.","title":"Capture from recorded video"},{"location":"other-settings/capture-from-recorded-video/#capture-from-recorded-video","text":"From v1.12, you can capture motion using recorded videos.","title":"Capture from recorded video"},{"location":"other-settings/capture-from-recorded-video/#how-to-use","text":"Select Recorded video from the \u25bc next to Add camera at the top of the MocapForAll window. Press the ... button and select the video file. Do not use multi-byte characters If the file path contains multi-byte characters (e.g. Japanese hiragana), the video file may fail to open. After that, the usage is basically the same as a normal webcam.","title":"How to use"},{"location":"other-settings/capture-from-recorded-video/#playback-positions","text":"The video will automatically loop. The playback position of the video will return to the beginning of the video when you press Start Capture .","title":"Playback positions"},{"location":"other-settings/capture-from-recorded-video/#tips","text":"It is recommended to separate the video files for intrinsic parameter calibration, extrinsic parameter calibration, and actual motion capture. There is no function to analyze the videos and synchronize the movements between them, so it is necessary to edit the videos in advance so that the movements will be synchronized when they are played at the same time from the beginning. When recording videos, it is recommended to clarify the starting point with a clapperboard or the sound of clapping your hands.","title":"Tips"},{"location":"other-settings/capture-hand-and-face/","text":"Capture hand and face Turn on Settings > General > Capture hand to capture finger movements Turn on Settings > General > Capture face to capture facial expressions Note that these are experimental features. The accuracy is low and CPU / GPU usage is high. Cropping size for hand / face You can adjust the crop size of the image used to capture your hands and face from Settings > Advanced > Cropping size for hand and Cropping size for face . Since different people have different hand and face sizes, adjusting these values may improve accuracy. Hand capture Full / Lite You can select the hand tracking model from Full, Lite or Legacy from the pulldown Settings > General > Capture hand . Full : More accurate. Uses more CPU/GPU. Lite : Less accurate. Uses less CPU/GPU. Legacy : Uses the model used in v1.13 and before. For backward compatibility. Specify facial morph target names You can specify the names of facial morph targets if you are using VRM models. Turn on Settings > Advanced > Specify facial morph target names Input morph target names","title":"Capture hand and face"},{"location":"other-settings/capture-hand-and-face/#capture-hand-and-face","text":"Turn on Settings > General > Capture hand to capture finger movements Turn on Settings > General > Capture face to capture facial expressions Note that these are experimental features. The accuracy is low and CPU / GPU usage is high.","title":"Capture hand and face"},{"location":"other-settings/capture-hand-and-face/#cropping-size-for-hand-face","text":"You can adjust the crop size of the image used to capture your hands and face from Settings > Advanced > Cropping size for hand and Cropping size for face . Since different people have different hand and face sizes, adjusting these values may improve accuracy.","title":"Cropping size for hand / face"},{"location":"other-settings/capture-hand-and-face/#hand-capture-full-lite","text":"You can select the hand tracking model from Full, Lite or Legacy from the pulldown Settings > General > Capture hand . Full : More accurate. Uses more CPU/GPU. Lite : Less accurate. Uses less CPU/GPU. Legacy : Uses the model used in v1.13 and before. For backward compatibility.","title":"Hand capture Full / Lite"},{"location":"other-settings/capture-hand-and-face/#specify-facial-morph-target-names","text":"You can specify the names of facial morph targets if you are using VRM models. Turn on Settings > Advanced > Specify facial morph target names Input morph target names","title":"Specify facial morph target names"},{"location":"other-settings/use-as-dummy-hmd-and-controllers/","text":"Use as dummy HMD and controllers Experimental feature You are using an experimental feature. Do not expect it to work . Disconnect all VR devices. Open C:\\Program Files (x86)\\Steam\\config\\steamvr.vrsettings . Add the following lines: \"TrackingOverrides\" : { \"/devices/vmt/VMT_9\" : \"/user/head\" }, \"driver_null\" : { \"enable\" : true, \"id\" : \"Null Driver\", \"windowHeight\" : 1080, \"windowWidth\" : 1920, \"windowX\" : 100, \"windowY\" : 100 }, See VMT manual for details of \"TrackingOverrides\" settings. Start SteamVR. Start MocapForAll and turn on export settings for VMT protocol. Set Orientation offsets for head and hands. Start capture. If you want to use buttons on controller, use VMTManager or try programs below. (You need to build them from source code.) VMT-RemoteController Modified VMT - AddJoystickTouchAndClick branch","title":"Use as dummy HMD and controllers"},{"location":"other-settings/use-as-dummy-hmd-and-controllers/#use-as-dummy-hmd-and-controllers","text":"Experimental feature You are using an experimental feature. Do not expect it to work . Disconnect all VR devices. Open C:\\Program Files (x86)\\Steam\\config\\steamvr.vrsettings . Add the following lines: \"TrackingOverrides\" : { \"/devices/vmt/VMT_9\" : \"/user/head\" }, \"driver_null\" : { \"enable\" : true, \"id\" : \"Null Driver\", \"windowHeight\" : 1080, \"windowWidth\" : 1920, \"windowX\" : 100, \"windowY\" : 100 }, See VMT manual for details of \"TrackingOverrides\" settings. Start SteamVR. Start MocapForAll and turn on export settings for VMT protocol. Set Orientation offsets for head and hands. Start capture. If you want to use buttons on controller, use VMTManager or try programs below. (You need to build them from source code.) VMT-RemoteController Modified VMT - AddJoystickTouchAndClick branch","title":"Use as dummy HMD and controllers"}]}